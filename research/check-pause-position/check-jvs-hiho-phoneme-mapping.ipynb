{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jVS_HIHOで付けたJVSの音素アライメントが、音声と一致しているか確認する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "import librosa.display\n",
    "from scipy import ndimage\n",
    "from IPython.display import Audio\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "data_dir = os.getenv(\"DATA_DIR\")\n",
    "jvs_hiho_dir = os.getenv(\"JVS_HIHO_DIR\")\n",
    "\n",
    "jvs_dir = Path(data_dir) / \"jvs_takeshun_ver1\"\n",
    "jvs_alignment_dir = Path(jvs_hiho_dir) / \"aligned_labels_julius_takehun\"\n",
    "jvs_transcirpt_path = Path(jvs_hiho_dir) / \"voiceactoress100_spaced_julius.txt\"\n",
    "\n",
    "print(f\"jvs_dir: {jvs_dir}\")\n",
    "print(f\"jvs_alignment_dir: {jvs_alignment_dir}\")\n",
    "print(f\"jvs_transcirpt_path: {jvs_transcirpt_path}\")\n",
    "assert jvs_dir.exists()\n",
    "assert jvs_alignment_dir.exists()\n",
    "assert jvs_transcirpt_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jvs_dir.resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_files = jvs_dir.glob(\"*/parallel100/*/*.wav\")\n",
    "list(wav_files)[0].parts[-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(\"002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 対応するファイルをデータフレームに読み込む\n",
    "df = []\n",
    "\n",
    "wav_files = jvs_dir.glob(\"*/parallel100/*/*.wav\")\n",
    "\n",
    "with open(jvs_transcirpt_path, \"r\") as f:\n",
    "    # １行ずつのリストにする。\n",
    "    lines = f.readlines()\n",
    "\n",
    "if len(lines) != 100:\n",
    "    print(f\"lines length is {len(lines)}.\")\n",
    "\n",
    "for wav_path in wav_files:\n",
    "    wav_path_parts = wav_path.parts\n",
    "    spk_id, _, _, wav_name = wav_path_parts[-4:]\n",
    "    wav_stem = Path(wav_name).stem\n",
    "\n",
    "    lab_path = jvs_alignment_dir / spk_id / wav_name.replace(\".wav\", \".lab\")\n",
    "    if not lab_path.exists():\n",
    "        print(f\"{lab_path} does not exist.\")\n",
    "\n",
    "    wav_id = int(wav_stem.split(\"_\")[-1])  # 1~100\n",
    "    transcript = lines[wav_id - 1]\n",
    "\n",
    "    df.append(\n",
    "        {\n",
    "            \"spk_id\": spk_id,\n",
    "            \"wav_id\": wav_id,\n",
    "            \"wav_stem\": wav_stem,\n",
    "            \"transcript\": transcript,\n",
    "            \"wav_path\": wav_path,\n",
    "            \"lab_path\": lab_path,\n",
    "        }\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "display(df.info())\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "df_tmp = df.iloc[:10]\n",
    "df_tmp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 便利な抽出関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 音声波形抽出\n",
    "# wav_path -> wav, sr\n",
    "def extract_waveform(audio_file_path, sr=24000):\n",
    "    waveform, sample_rate = librosa.load(audio_file_path, mono=True)\n",
    "    return waveform, sample_rate\n",
    "\n",
    "\n",
    "# wav -> db変換\n",
    "def convert_db(waveform):\n",
    "    db = librosa.power_to_db(waveform)\n",
    "    return db\n",
    "\n",
    "\n",
    "# 連続区間抽出\n",
    "# db -> bool_list\n",
    "def run_length_encoding(arr, min_run_length=3):\n",
    "    diff = np.diff(arr)  # 隣接要素の差分を計算\n",
    "    run_starts = np.where(diff != 0)[0] + 1  # 差分が0でないインデックスを取得し、連続する範囲の開始位置を得る\n",
    "    run_lengths = np.diff(np.concatenate(([0], run_starts, [len(arr)])))  # 連続する範囲の長さを計算\n",
    "    result = np.repeat(run_lengths >= min_run_length, run_lengths)  # 連続する範囲をTrueに変換\n",
    "    return result\n",
    "\n",
    "\n",
    "# Pause区間抽出\n",
    "# db, db_threshold, time_threshold, sr -> pause_bool_list\n",
    "# 閾値を超えたらpauseとみなす\n",
    "def detect_pause_position(\n",
    "    db_sequence, db_threshold=-50, time_threshold=50 / 1000, sample_rate=24000\n",
    "):\n",
    "    \"\"\"dbと音声長の閾値からpauseの位置を判定する。\n",
    "\n",
    "    Args:\n",
    "        db_sequence (np.array): 音声波形をdbに変換した配列\n",
    "        db_threshold (float): 無音区間とするdbの閾値\n",
    "        time_threshold (float): 無音区間が連続した時にpauseとみなす時間の閾値\n",
    "\n",
    "    Returns:\n",
    "        pause_positions (list): pauseの位置のリスト\n",
    "    \"\"\"\n",
    "    under_db_threshold = db_sequence < db_threshold\n",
    "\n",
    "    # 連続区間を抽出\n",
    "    sample_threshold = int(time_threshold * sample_rate)\n",
    "    is_continuous = run_length_encoding(under_db_threshold, sample_threshold)\n",
    "\n",
    "    # pauseの位置を抽出\n",
    "    pause_positions = under_db_threshold & is_continuous\n",
    "\n",
    "    return pause_positions\n",
    "\n",
    "\n",
    "# pause区間付きの波形の可視化\n",
    "def plot_db_with_pause(db, sr, db_threshold, time_threshold, xlim=None):\n",
    "    fig, ax = plt.subplots(figsize=(20, 5))\n",
    "    x = np.arange(len(db)) / sr\n",
    "    ax.plot(x, db, label=\"db\")\n",
    "\n",
    "    # dbの閾値を引く\n",
    "    ax.axhline(\n",
    "        y=db_threshold,\n",
    "        color=\"r\",\n",
    "        linestyle=\"-\",\n",
    "        linewidth=2,\n",
    "        alpha=0.7,\n",
    "        label=\"db_threshold\",\n",
    "    )\n",
    "\n",
    "    # pauseの領域を塗りつぶす\n",
    "    pause_position = detect_pause_position(db, db_threshold, time_threshold, sr)\n",
    "    plt.fill_between(x, -80, 0, where=pause_position, facecolor=\"b\", alpha=0.5)\n",
    "\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 波形の可視化\n",
    "def plot_wavform(waveform, sr, xlim=None):\n",
    "    fig, ax = plt.subplots(figsize=(20, 5))\n",
    "    x = np.arange(len(waveform)) / sr\n",
    "    ax.plot(x, waveform, label=\"waveform\")\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 音声再生ボタン生成\n",
    "def play_button(waveform, sr):\n",
    "    display(Audio(waveform, rate=sr, autoplay=True))\n",
    "\n",
    "\n",
    "# アライメントの抽出\n",
    "# lab_path -> df_lab\n",
    "def read_lab(lab_path):\n",
    "    \"\"\"labファイルを読み込む\"\"\"\n",
    "    # labファイルがない場合\n",
    "    if not Path(lab_path).exists():\n",
    "        print(f\"{lab_path} does not exist.\")\n",
    "        return None\n",
    "\n",
    "    # labファイルがある場合\n",
    "    df_lab = []\n",
    "    with open(lab_path, \"r\") as f:\n",
    "        for phoneme_idx, line in enumerate(f):\n",
    "            if line == \"\":\n",
    "                continue\n",
    "            start, end, phoneme = line.split()\n",
    "            duration = float(end) - float(start)\n",
    "            df_lab.append(\n",
    "                {\n",
    "                    \"start\": float(start),\n",
    "                    \"end\": float(end),\n",
    "                    \"phoneme\": phoneme,\n",
    "                    \"phoneme_idx\": phoneme_idx,\n",
    "                    \"duration\": duration,\n",
    "                }\n",
    "            )\n",
    "    df_lab = pd.DataFrame(df_lab)\n",
    "    return df_lab\n",
    "\n",
    "\n",
    "# アライメントの可視化\n",
    "def plot_phoneme_alignment(lab_path, xlim=None):\n",
    "    \"\"\"Labファイルから音素のアライメントをプロットする\n",
    "\n",
    "    Args:\n",
    "        lab_path (_type_): Labファイルのパス\n",
    "    \"\"\"\n",
    "    df = read_lab(lab_path)\n",
    "    display(df[-10:])\n",
    "\n",
    "    # 描画\n",
    "    fig, ax = plt.subplots(figsize=(20, 2))\n",
    "    for start, end, label, _, _ in df.values:\n",
    "        ax.axvline(start, color=\"gray\", linestyle=\"--\")\n",
    "        ax.axvline(end, color=\"gray\", linestyle=\"--\")\n",
    "        # ax.text((start + end) / 2, 0.5, label, ha='center', va='bottom', fontsize=20)\n",
    "        # ax.text(start + (end-start), 0.5, label, ha='center', va='bottom', fontsize=20)\n",
    "    # ax.set_yticks([])\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_xlabel(\"Time (seconds)\")\n",
    "    fig.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 並べて可視化する。\n",
    "def plot_all(\n",
    "    df_tmp_iloc, sample_rate=24000, db_threshold=-50, time_threshold=50 / 1000\n",
    "):\n",
    "    spk_id, wav_id, transcript, wav_path, lab_path = df_tmp_iloc[\n",
    "        \"spk_id wav_id transcript wav_path lab_path\".split(\" \")\n",
    "    ]\n",
    "    wav, sr = extract_waveform(wav_path, sr=sample_rate)\n",
    "    db = convert_db(wav)\n",
    "    xlim = (0, len(wav) / sr)\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        3, 1, figsize=(20, 10), gridspec_kw={\"height_ratios\": [4, 4, 2]}\n",
    "    )\n",
    "    print(\"spk_id:\", spk_id)\n",
    "    print(\"wav_id:\", wav_id)\n",
    "    print(\"xlim:\", xlim)\n",
    "    print(\"transcript:\", transcript)\n",
    "    print(\"start ploting...\")\n",
    "\n",
    "    # 波形の可視化\n",
    "    x = np.arange(len(wav)) / sr\n",
    "    ax[0].plot(x, wav, label=\"waveform\")\n",
    "    ax[0].set_xlim(xlim)\n",
    "    ax[0].legend()\n",
    "\n",
    "    # dbの可視化\n",
    "    x = np.arange(len(db)) / sr\n",
    "    ax[1].plot(x, db, label=\"db\")\n",
    "    # dbの閾値を引く\n",
    "    ax[1].axhline(\n",
    "        y=db_threshold,\n",
    "        color=\"r\",\n",
    "        linestyle=\"-\",\n",
    "        linewidth=2,\n",
    "        alpha=0.7,\n",
    "        label=\"db_threshold\",\n",
    "    )\n",
    "    # pauseの領域を塗りつぶす\n",
    "    pause_position = detect_pause_position(db, db_threshold, time_threshold, sr)\n",
    "    ax[1].fill_between(x, -80, 0, where=pause_position, facecolor=\"b\", alpha=0.5)\n",
    "    ax[1].set_xlim(xlim)\n",
    "    ax[1].legend()\n",
    "\n",
    "    # アライメントの可視化\n",
    "    df = read_lab(lab_path)\n",
    "    # 描画\n",
    "    for start, end, label, _, _ in df.values:\n",
    "        ax[2].axvline(start, color=\"gray\", linestyle=\"--\")\n",
    "        ax[2].axvline(end, color=\"gray\", linestyle=\"--\")\n",
    "        # ax.text((start + end) / 2, 0.5, label, ha='center', va='bottom', fontsize=20)\n",
    "        # ax.text(start + (end-start), 0.5, label, ha='center', va='bottom', fontsize=20)\n",
    "    # ax.set_yticks([])\n",
    "    ax[2].set_xlim(xlim)\n",
    "    ax[2].set_xlabel(\"Time (seconds)\")\n",
    "    # ax[2].tight_layout()\n",
    "    ax[2].legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(\"done.\")\n",
    "\n",
    "\n",
    "def classfy_pause(\n",
    "    db_sequence, lab_path, sample_rate=24000, db_threshold=-50, time_threshold=0.05\n",
    "):\n",
    "    \"\"\"ポーズを分類する\n",
    "\n",
    "    Args:\n",
    "        df_jvs (_type_): _description_\n",
    "    \"\"\"\n",
    "    # db_threshold = -50\n",
    "    # time_threshold = 0.05\n",
    "    # sample_rate = 24000\n",
    "\n",
    "    # db_sequence = df_jvs.iloc[0]['db_sequence']\n",
    "    pause_position = detect_pause_position(\n",
    "        db_sequence, db_threshold, time_threshold, sample_rate\n",
    "    )\n",
    "\n",
    "    def run_length_encoding_range(arr, min_run_length=3):\n",
    "        \"\"\"\n",
    "        Run-Length Encoding (RLE)を実行して連続している部分をTrueとしたブール配列を返す関数\n",
    "\n",
    "        Parameters:\n",
    "            arr (numpy.ndarray): 連続している部分を判定したい1次元のNumPy配列\n",
    "            min_run_length (int): 連続していると判定する最小の長さ（デフォルトは3）\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: 連続している部分がTrueとなったブール配列\n",
    "            list: 連続している部分の始点と終点のリスト [(start1, end1), (start2, end2), ...]\n",
    "        \"\"\"\n",
    "        diff = np.diff(arr)  # 隣接要素の差分を計算\n",
    "        run_starts = np.where(diff != 0)[0] + 1  # 差分が0でないインデックスを取得し、連続する範囲の開始位置を得る\n",
    "\n",
    "        starts = np.concatenate(([0], run_starts))\n",
    "        ends = np.concatenate((run_starts, [len(arr)]))\n",
    "        lengths = ends - starts\n",
    "        ranges = list(zip(starts, ends, lengths))\n",
    "\n",
    "        # min_run_length以下の範囲を削除, Trueが連続しているもののみを取り出す\n",
    "        ranges = [r for r in ranges if (r[2] >= min_run_length and arr[r[0]])]\n",
    "\n",
    "        return ranges\n",
    "\n",
    "    sample_threshold = int(time_threshold * sample_rate)\n",
    "    pause_ranges = run_length_encoding_range(pause_position, sample_threshold)\n",
    "\n",
    "    # print(pause_ranges)\n",
    "\n",
    "    # df_lab = read_lab(df_jvs.iloc[0]['lab_path'])\n",
    "    df_lab = read_lab(lab_path)\n",
    "\n",
    "    ans = []\n",
    "    for pause_range in pause_ranges:\n",
    "        # df_labのstartもしくは、endが、start, endの範囲内にあるかどうか\n",
    "        pause_start = pause_range[0]\n",
    "        pause_end = pause_range[1]\n",
    "        phoneme_start = df_lab[\"start\"].values * sample_rate\n",
    "        phoneme_end = df_lab[\"end\"].values * sample_rate\n",
    "        is_start_include = (pause_start <= phoneme_start) & (phoneme_start <= pause_end)\n",
    "        is_end_include = (pause_start <= phoneme_end) & (phoneme_end <= pause_end)\n",
    "\n",
    "        include_phonemes = df_lab[is_start_include | is_end_include][\"phoneme\"].values\n",
    "        print(include_phonemes)\n",
    "        if \"silE\" in include_phonemes:\n",
    "            pause_type = \"silE\"\n",
    "        elif \"silB\" in include_phonemes:\n",
    "            pause_type = \"silB\"\n",
    "        elif \"sil\" in include_phonemes:\n",
    "            pause_type = \"sil\"\n",
    "        elif \"pau\" in include_phonemes:\n",
    "            pause_type = \"pau\"\n",
    "        elif \"sp\" in include_phonemes:\n",
    "            pause_type = \"sp\"\n",
    "        else:\n",
    "            pause_type = \"RP\"\n",
    "\n",
    "        ans.append([pause_range[0], pause_range[1], pause_range[2], pause_type])\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可視化してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_threshold = -25  # dB\n",
    "time_threshold = 50 / 1000  # 50ms\n",
    "sample_rate = 24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### それぞれ可視化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_id, wav_id, transcript, wav_path, lab_path = df[df[\"spk_id\"] == \"jvs001\"].iloc[1][\n",
    "    \"spk_id wav_id transcript wav_path lab_path\".split(\" \")\n",
    "]\n",
    "\n",
    "wav, sr = extract_waveform(wav_path, sr=sample_rate)\n",
    "db = convert_db(wav)\n",
    "xlim = (0, len(wav) / sample_rate)\n",
    "\n",
    "print(\"spk_id:\", spk_id)\n",
    "print(\"wav_id:\", wav_id)\n",
    "print(\"xlim:\", xlim)\n",
    "print(\"transcript:\", transcript)\n",
    "print(\"start ploting...\")\n",
    "plot_wavform(wav, sr=sample_rate, xlim=xlim)\n",
    "plot_db_with_pause(\n",
    "    db,\n",
    "    sr=sample_rate,\n",
    "    db_threshold=db_threshold,\n",
    "    time_threshold=time_threshold,\n",
    "    xlim=xlim,\n",
    ")\n",
    "plot_phoneme_alignment(lab_path, xlim=xlim)\n",
    "play_button(wav, sr=sample_rate)\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "アライメントの文字を削除して、yticksを表示することで、x軸の縮尺が会うようにした。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### まとめて可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(\n",
    "    df[df[\"spk_id\"] == \"jvs001\"].iloc[1],\n",
    "    sample_rate=sample_rate,\n",
    "    db_threshold=db_threshold,\n",
    "    time_threshold=time_threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X軸の縮尺が会うようになったが、JVS_HIHOの影響か、dbが合わなくなった。  \n",
    "-> 波形が大体1/2になってそうなので、db_thresholdも半分にしてみる db_threshold=-25\n",
    "\n",
    "\n",
    "-> データの型を見る int float (librosaでの撮り方)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ポーズの分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"spk_id\"] == \"jvs001\"].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_id, wav_id, transcript, wav_path, lab_path = df[df[\"spk_id\"] == \"jvs001\"].iloc[1][\n",
    "    \"spk_id wav_id transcript wav_path lab_path\".split(\" \")\n",
    "]\n",
    "\n",
    "wav, sr = extract_waveform(wav_path, sr=sample_rate)\n",
    "db = convert_db(wav)\n",
    "\n",
    "classfy_pause(\n",
    "    db,\n",
    "    lab_path,\n",
    "    sample_rate=sample_rate,\n",
    "    db_threshold=db_threshold,\n",
    "    time_threshold=time_threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "あってないな..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_id, wav_id, transcript, wav_path, lab_path = df[df[\"spk_id\"] == \"jvs001\"].iloc[1][\n",
    "    \"spk_id wav_id transcript wav_path lab_path\".split(\" \")\n",
    "]\n",
    "\n",
    "wav, sr = extract_waveform(wav_path, sr=sample_rate)\n",
    "db = convert_db(wav)\n",
    "print(\"len\", len(db) / sample_rate)\n",
    "\n",
    "pause_bool_list = detect_pause_position(\n",
    "    db,\n",
    "    db_threshold=db_threshold,\n",
    "    time_threshold=time_threshold,\n",
    "    sample_rate=sample_rate,\n",
    ")\n",
    "pause_bool_list = np.array(pause_bool_list)\n",
    "\n",
    "df_lab = read_lab(lab_path)\n",
    "df_lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 現状"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JVS_HIHOの方法で生成したアライメント結果を使うことで、JVSの音声と、アライメントの対応を取ることができた。   \n",
    "- JVS_HIHOでは、JuliusをPythonから呼び出すような方法で、アライメントを生成している。->少し結果が異なるのかも\n",
    "- ただし、音声波形の絶対値が異なっており、dbの閾値が-50ではなく、-30を使うことにした。   \n",
    "閾値で抽出したポーズの分類関数がうまく実装できない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 各話者ごとのポーズの長さの平均値と、音声の長さで割った値の平均値を計算する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labファイルから、spのdurationの合計と、silBのendとsilEのstartの差を計算する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pause_length = []\n",
    "speech_length = []\n",
    "for i, row in df.iterrows():\n",
    "    lab_path = row[\"lab_path\"]\n",
    "    df_lab = read_lab(lab_path)\n",
    "    # silBのend ~ silEのstartまでの長さ\n",
    "    speech_len = df_lab.iloc[-1][\"end\"] - df_lab.iloc[0][\"start\"]\n",
    "    speech_length.append(speech_len)\n",
    "    # spのdurationの合計\n",
    "    pause_len = df_lab[df_lab[\"phoneme\"] == \"sp\"][\"duration\"].sum()\n",
    "    pause_length.append(pause_len)\n",
    "df[\"pause_length\"] = pause_length\n",
    "df[\"speech_length\"] = speech_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 話者ごとのポーズの長さの平均の分布\n",
    "plt.figure(figsize=(16, 8))\n",
    "df.groupby(\"spk_id\")[\"pause_length\"].mean().hist(bins=30, figsize=(16, 8))\n",
    "plt.title(\"Distribution of mean pause length by speaker\")\n",
    "plt.xlabel(\"Mean pause length\")\n",
    "plt.ylabel(\"Number of speakers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 話者ごとの音声のうち、ポーズの占める割合の平均の分布\n",
    "plt.figure(figsize=(16, 8))\n",
    "df_pause_ratio = df.groupby(\"spk_id\")[\"pause_length speech_length\".split()].sum()\n",
    "df_pause_ratio[\"pause_ratio\"] = (\n",
    "    df_pause_ratio[\"pause_length\"] / df_pause_ratio[\"speech_length\"]\n",
    ")\n",
    "df_pause_ratio[\"pause_ratio\"].hist(bins=30, figsize=(16, 8))\n",
    "plt.xlabel(\"Pause ratio\")\n",
    "plt.ylabel(\"Number of speakers\")\n",
    "plt.title(\"Pause ratio distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pause_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 列ごとのscatter plotを作成する\n",
    "\n",
    "df_pause_ratio_tmp = df_pause_ratio.rename(\n",
    "    columns={\"pause_length\": \"pause_length_sum\", \"speech_length\": \"speech_length_sum\"}\n",
    ")\n",
    "sns.pairplot(df_pause_ratio_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ポーズの長さと発声の長さは少し相関がありそう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 発話速度を基準として、相対的なポーズの長さをとる。-> 平均発話速度に対して、どれくらいのポーズの長さを\n",
    "- スタイルごとに、ポーズのプロファイリングや分布をとる。\n",
    "- 朗読のコーパスで、複数人で、各個人のポーズの取り方を再現する。予測する。\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jvs_data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
