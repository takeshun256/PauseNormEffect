{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ポーズの分布の谷である250ms前後の音声を聞く"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  閾値を設けない場合の「ポーズ長」の分布のグラフで，ゼロ秒の鋭いピークを中心とした分布と．0.5 秒付近のピークを中心とした分布がかなりの程度重なり合っているのがやはり気になります．先行研究でも適当に決めた閾値を超えるかどうかで「ポーズあり・なし」の学習/評価データを作っているようですが，閾値（たとえば250ms）前後のポーズの部分を聞いたときにどのように聞こえるのでしょうか．周辺単語によって「ポーズがある」と感じたりそうでなかったりするか？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "500msくらいは読点のポーズとして聞こえるが，250msくらいはどういうポーズがあるか実際に聞いてみて調査する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "from matplotlib import pyplot as plt\n",
    "import japanize_matplotlib\n",
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "\n",
    "# conda activate pyopenjtalk_julius\n",
    "\n",
    "# set path\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/takeshun256/PausePrediction\")\n",
    "\n",
    "# import standard library\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pandas as pd\n",
    "\n",
    "# import pyopenjtalk\n",
    "# from pyopenjtalk import run_frontend, g2p\n",
    "import jaconv\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import own library\n",
    "from config import DATA_DIR, DATA_TAKESHUN256_DIR, SRC_DIR, DATA_IN_ROOT_DIR\n",
    "\n",
    "# from src.analyze_jmac.text_preprocessing import (\n",
    "#     AudiobookScriptPreprocessor as Preprocessor,\n",
    "# )\n",
    "from src.analyze_jmac.mecab import mecab_wakati_generator, mecab_detailed_generator\n",
    "\n",
    "# define path\n",
    "corpus_name = \"jmac\"\n",
    "exp_name = \"03_VAD_Adjusted\"\n",
    "\n",
    "exp_dir = Path(DATA_TAKESHUN256_DIR) / corpus_name / exp_name\n",
    "yaml_file_path = Path(DATA_IN_ROOT_DIR) / corpus_name / \"text_audio_dict_new.yaml\"\n",
    "\n",
    "assert exp_dir.exists()\n",
    "assert yaml_file_path.exists()\n",
    "\n",
    "import csv\n",
    "import yaml\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/takeshun256/PausePrediction/src/analyze_jmac\")\n",
    "sys.path.append(\"/home/takeshun256/PausePrediction/src/vad_tool\")\n",
    "from audiobook_yaml_parser import extract_yaml_data\n",
    "from py_webrtcvad_test import getVadSection\n",
    "from vad_tool import VAD_Segmenter\n",
    "from audiobook_dataset_builder import AudiobookDatasetBuilder\n",
    "from audiobook_script_extractor import AudiobookScriptExtractor\n",
    "\n",
    "# python==3.11以上の場合はimportしない\n",
    "# if sys.version_info < (3, 11):\n",
    "#     from text_preprocessing_preprocessor import AudiobookScriptPreprocessor\n",
    "# from text_preprocessing import AudiobookScriptPreprocessor\n",
    "from julius_lab_analysis import JuliusLabAnalyzer\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "from scipy.io import wavfile\n",
    "import scipy.io.wavfile\n",
    "import scipy.ndimage\n",
    "import scipy.signal\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import japanize_matplotlib\n",
    "import webrtcvad\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import struct\n",
    "import librosa.display\n",
    "from IPython.display import Audio\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Dict, Any, Union\n",
    "import soundfile as sf\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 音声波形抽出\n",
    "# wav_path -> wav, sr\n",
    "def extract_waveform(audio_file_path, sr=24000):\n",
    "    waveform, sample_rate = librosa.load(audio_file_path, sr=sr, mono=True)\n",
    "    return waveform, sample_rate\n",
    "\n",
    "\n",
    "# wav -> db変換\n",
    "def convert_db(waveform):\n",
    "    # TODO: これは何が違うのか？どちらが適切か？\n",
    "    # db = librosa.power_to_db(waveform)\n",
    "    db = librosa.amplitude_to_db(waveform, ref=np.max)\n",
    "    return db\n",
    "\n",
    "\n",
    "# 連続区間抽出\n",
    "# db -> bool_list\n",
    "def run_length_encoding(arr, min_run_length=3):\n",
    "    diff = np.diff(arr)  # 隣接要素の差分を計算\n",
    "    run_starts = np.where(diff != 0)[0] + 1  # 差分が0でないインデックスを取得し、連続する範囲の開始位置を得る\n",
    "    run_lengths = np.diff(np.concatenate(([0], run_starts, [len(arr)])))  # 連続する範囲の長さを計算\n",
    "    result = np.repeat(run_lengths >= min_run_length, run_lengths)  # 連続する範囲をTrueに変換\n",
    "    return result\n",
    "\n",
    "def run_length_encoding_range(arr, min_run_length=3):\n",
    "    diff = np.diff(arr)\n",
    "    run_starts = np.where(diff != 0)[0] + 1\n",
    "    run_lengths = np.diff(np.concatenate(([0], run_starts, [len(arr)])))\n",
    "    # 連続する範囲の開始、終了インデックスと長さを計算\n",
    "    runs = np.concatenate(([0], run_starts))\n",
    "    ranges_with_length = [(start, start + length, length) for start, length in zip(runs, run_lengths) \n",
    "                        if length >= min_run_length and arr[start]]\n",
    "    return ranges_with_length\n",
    "\n",
    "# Pause区間抽出\n",
    "# db, db_threshold, time_threshold, sr -> pause_bool_list\n",
    "# 閾値を超えたらpauseとみなす\n",
    "def detect_pause_position(\n",
    "    db_sequence, db_threshold=-50, time_threshold=50 / 1000, sample_rate=24000\n",
    "):\n",
    "    \"\"\"dbと音声長の閾値からpauseの位置を判定する。\n",
    "\n",
    "    Args:\n",
    "        db_sequence (np.array): 音声波形をdbに変換した配列\n",
    "        db_threshold (float): 無音区間とするdbの閾値\n",
    "        time_threshold (float): 無音区間が連続した時にpauseとみなす時間の閾値\n",
    "\n",
    "    Returns:\n",
    "        pause_positions (list): pauseの位置のリスト\n",
    "    \"\"\"\n",
    "    under_db_threshold = db_sequence < db_threshold\n",
    "\n",
    "    # 連続区間を抽出\n",
    "    sample_threshold = int(time_threshold * sample_rate)\n",
    "    is_continuous = run_length_encoding(under_db_threshold, sample_threshold)\n",
    "\n",
    "    # pauseの位置を抽出\n",
    "    pause_positions = under_db_threshold & is_continuous\n",
    "\n",
    "    return pause_positions\n",
    "\n",
    "\n",
    "# pause区間付きの波形の可視化\n",
    "def plot_db_with_pause(db, sr, db_threshold, time_threshold, xlim=None):\n",
    "    fig, ax = plt.subplots(figsize=(20, 5))\n",
    "    x = np.arange(len(db)) / sr\n",
    "    ax.plot(x, db, label=\"db\")\n",
    "\n",
    "    # dbの閾値を引く\n",
    "    ax.axhline(\n",
    "        y=db_threshold,\n",
    "        color=\"r\",\n",
    "        linestyle=\"-\",\n",
    "        linewidth=2,\n",
    "        alpha=0.7,\n",
    "        label=\"db_threshold\",\n",
    "    )\n",
    "\n",
    "    # pauseの領域を塗りつぶす\n",
    "    pause_position = detect_pause_position(db, db_threshold, time_threshold, sr)\n",
    "    plt.fill_between(x, -80, 0, where=pause_position, facecolor=\"b\", alpha=0.5)\n",
    "\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 波形の可視化\n",
    "def plot_wavform(waveform, sr, xlim=None):\n",
    "    fig, ax = plt.subplots(figsize=(20, 5))\n",
    "    x = np.arange(len(waveform)) / sr\n",
    "    ax.plot(x, waveform, label=\"waveform\")\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 音声再生ボタン生成\n",
    "def play_button(waveform, sr):\n",
    "    display(Audio(waveform, rate=sr, autoplay=True))\n",
    "\n",
    "\n",
    "# アライメントの抽出\n",
    "# lab_path -> df_lab\n",
    "def read_lab(lab_path):\n",
    "    \"\"\"labファイルを読み込む\"\"\"\n",
    "    # labファイルがない場合\n",
    "    if not Path(lab_path).exists():\n",
    "        print(f\"{lab_path} does not exist.\")\n",
    "        return None\n",
    "\n",
    "    # labファイルがある場合\n",
    "    df_lab = []\n",
    "    with open(lab_path, \"r\") as f:\n",
    "        for phoneme_idx, line in enumerate(f):\n",
    "            if line == \"\":\n",
    "                continue\n",
    "            start, end, phoneme = line.split()\n",
    "            duration = float(end) - float(start)\n",
    "            df_lab.append(\n",
    "                {\n",
    "                    \"start\": float(start),\n",
    "                    \"end\": float(end),\n",
    "                    \"phoneme\": phoneme,\n",
    "                    \"phoneme_idx\": phoneme_idx,\n",
    "                    \"duration\": duration,\n",
    "                }\n",
    "            )\n",
    "    df_lab = pd.DataFrame(df_lab)\n",
    "    return df_lab\n",
    "\n",
    "\n",
    "# アライメントの可視化\n",
    "def plot_phoneme_alignment(df, xlim=None):\n",
    "    \"\"\"Labファイルから音素のアライメントをプロットする\n",
    "\n",
    "    Args:\n",
    "        lab_path (_type_): Labファイルのパス\n",
    "    \"\"\"\n",
    "    # df = read_lab(lab_path)\n",
    "    # display(df[-10:])\n",
    "\n",
    "    # 描画\n",
    "    fig, ax = plt.subplots(figsize=(20, 2))\n",
    "    for start, end, label in df.values:\n",
    "        ax.axvline(start, color=\"gray\", linestyle=\"--\")\n",
    "        ax.text((start + end) / 2, 0.5, label, ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "    # ax.set_yticks([])\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_xlabel(\"Time (seconds)\")\n",
    "    fig.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 並べて可視化する。\n",
    "def plot_all(\n",
    "    df_temp, wav_path, sample_rate=24000, db_threshold=-50, time_threshold=50 / 1000\n",
    "):\n",
    "    \"\"\"wavファイル、db、アライメントを並べて可視化する\"\"\"\n",
    "    wav, sr = extract_waveform(wav_path, sr=sample_rate)\n",
    "    db = convert_db(wav)\n",
    "    xlim = (0, len(wav) / sr)\n",
    "\n",
    "    print(wav_path)\n",
    "    print(\"wav.shape:\", wav.shape)\n",
    "    print(\"seconds:\", len(wav) / sr)\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        3, 1, figsize=(20, 10), gridspec_kw={\"height_ratios\": [4, 4, 2]}\n",
    "    )\n",
    "    # print(\"spk_id:\", spk_id)\n",
    "    # print(\"wav_id:\", wav_id)\n",
    "    # print(\"xlim:\", xlim)\n",
    "    # print(\"transcript:\", transcript)\n",
    "    # print(\"start ploting...\")\n",
    "\n",
    "    # 波形の可視化\n",
    "    x = np.arange(len(wav)) / sr\n",
    "    ax[0].plot(x, wav, label=\"waveform\")\n",
    "    ax[0].set_xlim(xlim)\n",
    "    ax[0].legend()\n",
    "\n",
    "    # dbの可視化\n",
    "    x = np.arange(len(db)) / sr\n",
    "    ax[1].plot(x, db, label=\"db\")\n",
    "    # dbの閾値を引く\n",
    "    ax[1].axhline(\n",
    "        y=db_threshold,\n",
    "        color=\"r\",\n",
    "        linestyle=\"-\",\n",
    "        linewidth=2,\n",
    "        alpha=0.7,\n",
    "        label=\"db_threshold\",\n",
    "    )\n",
    "    # pauseの領域を塗りつぶす\n",
    "    pause_position = detect_pause_position(db, db_threshold, time_threshold, sr)\n",
    "    ax[1].fill_between(x, -80, 0, where=pause_position, facecolor=\"b\", alpha=0.5)\n",
    "    ax[1].set_xlim(xlim)\n",
    "    ax[1].legend()\n",
    "\n",
    "    # アライメントの可視化\n",
    "    # 描画\n",
    "    for start, end, label in df_temp.values:\n",
    "        ax[2].axvline(end, color=\"gray\", linestyle=\"--\")\n",
    "        ax[2].axvline(start, color=\"gray\", linestyle=\"--\")\n",
    "        ax[2].text((start + end) / 2, 0.5, label, ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "    # ax.set_yticks([])\n",
    "    ax[2].set_xlim(xlim)\n",
    "    ax[2].set_xlabel(\"Time (seconds)\")\n",
    "    # ax[2].tight_layout()\n",
    "    ax[2].legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    play_button(wav, sr)\n",
    "\n",
    "\n",
    "# 並べて可視化する。\n",
    "def plot_all2(\n",
    "    df_temp, wav_path, sample_rate=24000, db_threshold=-50, time_threshold=50 / 1000\n",
    "):\n",
    "    \"\"\"wavファイル、db、アライメントを並べて可視化する\"\"\"\n",
    "    wav, sr = extract_waveform(wav_path, sr=sample_rate)\n",
    "    db = convert_db(wav)\n",
    "    xlim = (0, len(wav) / sr)\n",
    "\n",
    "    print(wav_path)\n",
    "    print(\"wav.shape:\", wav.shape)\n",
    "    print(\"seconds:\", len(wav) / sr)\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        2, 1, figsize=(20, 10), gridspec_kw={\"height_ratios\": [4, 4]}\n",
    "    )\n",
    "    # print(\"spk_id:\", spk_id)\n",
    "    # print(\"wav_id:\", wav_id)\n",
    "    # print(\"xlim:\", xlim)\n",
    "    # print(\"transcript:\", transcript)\n",
    "    # print(\"start ploting...\")\n",
    "\n",
    "    # 波形の可視化\n",
    "    x = np.arange(len(wav)) / sr\n",
    "    ax[0].plot(x, wav, label=\"waveform\")\n",
    "    ax[0].set_xlim(xlim)\n",
    "    ax[0].legend()\n",
    "\n",
    "    # dbの可視化\n",
    "    x = np.arange(len(db)) / sr\n",
    "    ax[1].plot(x, db, label=\"db\")\n",
    "    # dbの閾値を引く\n",
    "    ax[1].axhline(\n",
    "        y=db_threshold,\n",
    "        color=\"r\",\n",
    "        linestyle=\"-\",\n",
    "        linewidth=2,\n",
    "        alpha=0.7,\n",
    "        label=\"db_threshold\",\n",
    "    )\n",
    "    # pauseの領域を塗りつぶす\n",
    "    pause_position = detect_pause_position(db, db_threshold, time_threshold, sr)\n",
    "    ax[1].fill_between(x, -80, 0, where=pause_position, facecolor=\"b\", alpha=0.5)\n",
    "    ax[1].set_xlim(xlim)\n",
    "    ax[1].legend()\n",
    "\n",
    "    # アライメントの可視化\n",
    "    # 描画\n",
    "    for start, end, label in df_temp.values:\n",
    "        ax[0].axvline(end, color=\"gray\", linestyle=\"--\")\n",
    "        ax[0].axvline(start, color=\"gray\", linestyle=\"--\")\n",
    "        ax[0].text((start + end) / 2, 0.5, label, ha=\"center\", va=\"bottom\", fontsize=12)\n",
    "    # ax.set_yticks([])\n",
    "    ax[0].set_xlim(xlim)\n",
    "    ax[0].set_xlabel(\"Time (seconds)\")\n",
    "    # ax[2].tight_layout()\n",
    "    ax[0].legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    play_button(wav, sr)\n",
    "\n",
    "\n",
    "def get_pause_ranges(\n",
    "    db_sequence, db_threshold=-50, time_threshold=0.05, sample_rate=24000\n",
    "):\n",
    "    pause_position = detect_pause_position(\n",
    "        db_sequence, db_threshold, time_threshold, sample_rate\n",
    "    )\n",
    "\n",
    "    # def run_length_encoding_range(arr, min_run_length=3):\n",
    "    #     \"\"\"\n",
    "    #     Run-Length Encoding (RLE)を実行して連続している部分をTrueとしたブール配列を返す関数\n",
    "\n",
    "    #     Parameters:\n",
    "    #         arr (numpy.ndarray): 連続している部分を判定したい1次元のNumPy配列\n",
    "    #         min_run_length (int): 連続していると判定する最小の長さ（デフォルトは3）\n",
    "\n",
    "    #     Returns:\n",
    "    #         numpy.ndarray: 連続している部分がTrueとなったブール配列\n",
    "    #         list: 連続している部分の始点と終点のリスト [(start1, end1), (start2, end2), ...]\n",
    "    #     \"\"\"\n",
    "    #     diff = np.diff(arr)  # 隣接要素の差分を計算\n",
    "    #     run_starts = np.where(diff != 0)[0] + 1  # 差分が0でないインデックスを取得し、連続する範囲の開始位置を得る\n",
    "\n",
    "    #     starts = np.concatenate(([0], run_starts))\n",
    "    #     ends = np.concatenate((run_starts, [len(arr)]))\n",
    "    #     lengths = ends - starts\n",
    "    #     ranges = list(zip(starts, ends, lengths))\n",
    "\n",
    "    #     # min_run_length以下の範囲を削除, Trueが連続しているもののみを取り出す\n",
    "    #     ranges = [r for r in ranges if (r[2] >= min_run_length and arr[r[0]])]\n",
    "\n",
    "    #     return ranges\n",
    "\n",
    "\n",
    "    sample_threshold = int(time_threshold * sample_rate)\n",
    "    pause_ranges = run_length_encoding_range(pause_position, sample_threshold)\n",
    "    return pause_ranges\n",
    "\n",
    "\n",
    "def classfy_pause(\n",
    "    db_sequence, lab_path, sample_rate=24000, db_threshold=-50, time_threshold=0.05\n",
    "):\n",
    "    \"\"\"ポーズを分類する\n",
    "\n",
    "    Args:\n",
    "        df_jvs (_type_): _description_\n",
    "    \"\"\"\n",
    "    # db_threshold = -50\n",
    "    # time_threshold = 0.05\n",
    "    # sample_rate = 24000\n",
    "\n",
    "    # db_sequence = df_jvs.iloc[0]['db_sequence']\n",
    "    pause_position = detect_pause_position(\n",
    "        db_sequence, db_threshold, time_threshold, sample_rate\n",
    "    )\n",
    "\n",
    "    # def run_length_encoding_range(arr, min_run_length=3):\n",
    "    #     \"\"\"\n",
    "    #     Run-Length Encoding (RLE)を実行して連続している部分をTrueとしたブール配列を返す関数\n",
    "\n",
    "    #     Parameters:\n",
    "    #         arr (numpy.ndarray): 連続している部分を判定したい1次元のNumPy配列\n",
    "    #         min_run_length (int): 連続していると判定する最小の長さ（デフォルトは3）\n",
    "\n",
    "    #     Returns:\n",
    "    #         numpy.ndarray: 連続している部分がTrueとなったブール配列\n",
    "    #         list: 連続している部分の始点と終点のリスト [(start1, end1), (start2, end2), ...]\n",
    "    #     \"\"\"\n",
    "    #     diff = np.diff(arr)  # 隣接要素の差分を計算\n",
    "    #     run_starts = np.where(diff != 0)[0] + 1  # 差分が0でないインデックスを取得し、連続する範囲の開始位置を得る\n",
    "\n",
    "    #     starts = np.concatenate(([0], run_starts))\n",
    "    #     ends = np.concatenate((run_starts, [len(arr)]))\n",
    "    #     lengths = ends - starts\n",
    "    #     ranges = list(zip(starts, ends, lengths))\n",
    "\n",
    "    #     # min_run_length以下の範囲を削除, Trueが連続しているもののみを取り出す\n",
    "    #     ranges = [r for r in ranges if (r[2] >= min_run_length and arr[r[0]])]\n",
    "\n",
    "    #     return ranges\n",
    "\n",
    "    sample_threshold = int(time_threshold * sample_rate)\n",
    "    pause_ranges = run_length_encoding_range(pause_position, sample_threshold)\n",
    "\n",
    "    # print(pause_ranges)\n",
    "\n",
    "    # df_lab = read_lab(df_jvs.iloc[0]['lab_path'])\n",
    "    df_lab = read_lab(lab_path)\n",
    "\n",
    "    ans = []\n",
    "    for pause_range in pause_ranges:\n",
    "        # df_labのstartもしくは、endが、start, endの範囲内にあるかどうか\n",
    "        pause_start = pause_range[0]\n",
    "        pause_end = pause_range[1]\n",
    "        phoneme_start = df_lab[\"start\"].values * sample_rate\n",
    "        phoneme_end = df_lab[\"end\"].values * sample_rate\n",
    "        is_start_include = (pause_start <= phoneme_start) & (phoneme_start <= pause_end)\n",
    "        is_end_include = (pause_start <= phoneme_end) & (phoneme_end <= pause_end)\n",
    "\n",
    "        include_phonemes = df_lab[is_start_include | is_end_include][\"phoneme\"].values\n",
    "        print(include_phonemes)\n",
    "        if \"silE\" in include_phonemes:\n",
    "            pause_type = \"silE\"\n",
    "        elif \"silB\" in include_phonemes:\n",
    "            pause_type = \"silB\"\n",
    "        elif \"sil\" in include_phonemes:\n",
    "            pause_type = \"sil\"\n",
    "        elif \"pau\" in include_phonemes:\n",
    "            pause_type = \"pau\"\n",
    "        elif \"sp\" in include_phonemes:\n",
    "            pause_type = \"sp\"\n",
    "        else:\n",
    "            pause_type = \"RP\"\n",
    "\n",
    "        ans.append([pause_range[0], pause_range[1], pause_range[2], pause_type])\n",
    "    return ans\n",
    "\n",
    "def extract_pause_ranges_from_wavpath(\n",
    "    wav_path, sample_rate=24000, db_threshold=-50, time_threshold=50 / 1000\n",
    "):\n",
    "    \"\"\"wavファイルからpauseの位置を抽出する\"\"\"\n",
    "\n",
    "    wav, sr = extract_waveform(wav_path, sr=sample_rate)\n",
    "    db = convert_db(wav)\n",
    "\n",
    "    pause_position = detect_pause_position(\n",
    "        db, db_threshold, time_threshold, sample_rate\n",
    "    )\n",
    "\n",
    "    sample_threshold = int(time_threshold * sample_rate)\n",
    "    pause_ranges = run_length_encoding_range(pause_position, sample_threshold)\n",
    "    return pause_ranges\n",
    "\n",
    "\n",
    "# ----setting----\n",
    "# 閾値の設定\n",
    "db_threshold = -30\n",
    "# time_threshold = 50 / 1000 # 50ms\n",
    "time_threshold = 100 / 1000  # 200ms\n",
    "sample_rate = 24000\n",
    "# ---------------\n",
    "wav_p = \"/data2/takeshun256/jmac_split_and_added_lab/audiobook_25/audiobook_25_109.wav\"\n",
    "wav, sr = extract_waveform(wav_p, sr=sample_rate)\n",
    "db = convert_db(wav)\n",
    "plot_wavform(wav, sr)\n",
    "plot_db_with_pause(db, sr, db_threshold, time_threshold)\n",
    "pause_ranges = get_pause_ranges(db, db_threshold, time_threshold, sample_rate)\n",
    "print(pause_ranges)\n",
    "pause_ranges_by_wav = extract_pause_ranges_from_wavpath(wav_p, sample_rate, db_threshold, time_threshold)\n",
    "print(pause_ranges_by_wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAB_DIR = \"/data2/takeshun256/jmac_split_and_added_lab\"\n",
    "assert os.path.exists(LAB_DIR)\n",
    "# ディレクトリ内のlabファイルを取得\n",
    "lab_files = list(Path(LAB_DIR).glob(\"**/*.labm\"))\n",
    "\n",
    "print(\"labファイル数:\", len(list(lab_files)))\n",
    "print(\"labファイル例:\", list(lab_files)[0])\n",
    "\n",
    "# labファイルを読み込み\n",
    "lab_analyzer = JuliusLabAnalyzer(lab_files)\n",
    "\n",
    "\n",
    "fpath = exp_dir / \"pause_ranges\" / \"rule_based\" / \"df_lab_attached_morph_pause_rule_based_detected_-30_0.1.pkl\"\n",
    "\n",
    "import pickle\n",
    "with open(fpath, \"rb\") as f:\n",
    "    df_lab_attached = pickle.load(f)\n",
    "\n",
    "df_lab_attached = lab_analyzer.attach_speaker_info(df_lab_attached)\n",
    "\n",
    "print(df_lab_attached.shape)\n",
    "print(df_lab_attached.columns)\n",
    "display(df_lab_attached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各 audiobook ごとの、文中ポーズの長さの平均を計算する\n",
    "# 文章ごとにb雲中ポーズの長さを計算\n",
    "\n",
    "\n",
    "db_threshold = -30\n",
    "time_threshold = 100 / 1000\n",
    "\n",
    "threshold_str = f\"_{db_threshold}_{time_threshold}\"\n",
    "print(threshold_str)\n",
    "\n",
    "\n",
    "print(\"文章数\", df_lab_attached[\"lab_idx\"].nunique())\n",
    "df_pause_between_sentences = df_lab_attached.groupby(\n",
    "    [\n",
    "        \"audiobook_id\",\n",
    "        \"audiobook_id_int\",\n",
    "\n",
    "        \"chapter_id_int\",\n",
    "        \"lab_idx\",\n",
    "    ]\n",
    ").agg(\n",
    "    {\n",
    "        \"rule_based_detected_pause_ranges_in_sentence\" + threshold_str: lambda x: np.unique(x)[0],\n",
    "    }\n",
    ")\n",
    "df_pause_between_sentences = df_pause_between_sentences.reset_index()\n",
    "\n",
    "print(df_pause_between_sentences.shape)\n",
    "\n",
    "\n",
    "# リストの各要素を新たな行として追加\n",
    "def ready_explode(x):\n",
    "    if len(x) == 0:\n",
    "        return []\n",
    "    else:\n",
    "        return [xx[2] / 24000 for xx in x]\n",
    "\n",
    "\n",
    "df_pause_between_sentences[\n",
    "    \"rule_based_detected_pause_ranges_in_sentence\" + threshold_str\n",
    "] = df_pause_between_sentences[\"rule_based_detected_pause_ranges_in_sentence\" + threshold_str].apply(\n",
    "    ready_explode\n",
    ")\n",
    "df_pause_between_sentences = df_pause_between_sentences.explode(\n",
    "    \"rule_based_detected_pause_ranges_in_sentence\" + threshold_str\n",
    ")\n",
    "\n",
    "# rule_based_detected_pause_ranges_in_sentenceがNanの行を削除\n",
    "df_pause_between_sentences = df_pause_between_sentences[\n",
    "    ~df_pause_between_sentences[\"rule_based_detected_pause_ranges_in_sentence\" + threshold_str].isnull()\n",
    "]\n",
    "\n",
    "print(df_pause_between_sentences.shape)\n",
    "display(df_pause_between_sentences.head())\n",
    "\n",
    "\n",
    "# 分布\n",
    "# durationの分布を確認\n",
    "col = \"rule_based_detected_pause_ranges_in_sentence\" + threshold_str\n",
    "plt.figure(figsize=(15, 5))\n",
    "thres_time = 4\n",
    "df_tmp = df_pause_between_sentences[df_pause_between_sentences[col] < thres_time]\n",
    "sns.histplot(df_tmp[col], bins=100)\n",
    "plt.title(f\"{col} duration in sentence distribution (< {thres_time})\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pause_between_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# durationの分布を確認\n",
    "col = \"rule_based_detected_pause_ranges_in_sentence\" + threshold_str\n",
    "plt.figure(figsize=(15, 5))\n",
    "thres_time = 4\n",
    "df_tmp = df_pause_between_sentences[df_pause_between_sentences[col] < thres_time]\n",
    "sns.histplot(df_tmp.query(\"audiobook_id == 'audiobook_0'\")[col], bins=100)\n",
    "plt.title(f\"{col} duration in sentence distribution (< {thres_time}) in audiobook_0\")\n",
    "plt.xlim(0, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# durationの分布を確認\n",
    "col = \"rule_based_detected_pause_ranges_in_sentence\" + threshold_str\n",
    "plt.figure(figsize=(15, 5))\n",
    "thres_time = 4\n",
    "df_tmp = df_pause_between_sentences[df_pause_between_sentences[col] < thres_time]\n",
    "sns.histplot(df_tmp.query(\"audiobook_id == 'audiobook_41'\")[col], bins=100)\n",
    "plt.title(f\"{col} duration in sentence distribution (< {thres_time}) in audiobook_41\")\n",
    "plt.xlim(0, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# durationの分布を確認\n",
    "col = \"rule_based_detected_pause_ranges_in_sentence\" + threshold_str\n",
    "plt.figure(figsize=(15, 5))\n",
    "thres_time = 4\n",
    "df_tmp = df_pause_between_sentences[df_pause_between_sentences[col] < thres_time]\n",
    "sns.histplot(df_tmp.query(\"audiobook_id == 'audiobook_25'\")[col], bins=100)\n",
    "plt.title(f\"{col} duration in sentence distribution (< {thres_time}) in audiobook_25\")\n",
    "plt.xlim(0, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# durationの分布を確認\n",
    "col = \"rule_based_detected_pause_ranges_in_sentence\" + threshold_str\n",
    "plt.figure(figsize=(15, 5))\n",
    "thres_time = 4\n",
    "df_tmp = df_pause_between_sentences[df_pause_between_sentences[col] < thres_time]\n",
    "sns.histplot(df_tmp.query(\"audiobook_id == 'audiobook_60'\")[col], bins=100)\n",
    "plt.title(f\"{col} duration in sentence distribution (< {thres_time}) in audiobook_60\")\n",
    "plt.xlim(0, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# durationの分布を確認\n",
    "col = \"rule_based_detected_pause_ranges_in_sentence\" + threshold_str\n",
    "plt.figure(figsize=(15, 5))\n",
    "thres_time = 4\n",
    "df_tmp = df_pause_between_sentences[df_pause_between_sentences[col] < thres_time]\n",
    "sns.histplot(df_tmp.query(\"audiobook_id == 'audiobook_14'\")[col], bins=100)\n",
    "plt.title(f\"{col} duration in sentence distribution (< {thres_time}), in audiobook_14\")\n",
    "plt.xlim(0, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# durationの分布を確認\n",
    "col = \"rule_based_detected_pause_ranges_in_sentence\" + threshold_str\n",
    "plt.figure(figsize=(15, 5))\n",
    "thres_time = 4\n",
    "df_tmp = df_pause_between_sentences[df_pause_between_sentences[col] < thres_time]\n",
    "sns.histplot(df_tmp.query(\"audiobook_id == 'audiobook_34'\")[col], bins=100)\n",
    "plt.title(f\"{col} duration in sentence distribution (< {thres_time}) in audiobook_34\")\n",
    "plt.xlim(0, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aids = [41, 56, 69, 47, 2, 27, 34, 53, 14]\n",
    "\n",
    "for aid in aids:\n",
    "    audiobook = f\"audiobook_{aid}\"\n",
    "    # durationの分布を確認\n",
    "    col = \"rule_based_detected_pause_ranges_in_sentence\" + threshold_str\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    thres_time = 4\n",
    "    df_tmp = df_pause_between_sentences[df_pause_between_sentences[col] < thres_time]\n",
    "    sns.histplot(df_tmp.query(f\"audiobook_id == '{audiobook}'\")[col], bins=100)\n",
    "    plt.title(f\"{col} duration in sentence distribution (< {thres_time}) in {audiobook}\")\n",
    "    plt.xlim(0, 4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 250ms前後の音声を聞く"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_from_aid_chap(audiobook_id, chapter_id_int, df_lab_attached):\n",
    "    dir_path = Path(\"/data2/takeshun256/jmac_split_and_added_lab\")\n",
    "    wav_path = dir_path / audiobook_id / f\"{audiobook_id}_{chapter_id_int:03d}.wav\"\n",
    "    lab_path = dir_path / audiobook_id / f\"{audiobook_id}_{chapter_id_int:03d}.labm\"\n",
    "    \n",
    "    assert lab_path.exists()\n",
    "    assert wav_path.exists()\n",
    "    \n",
    "    sr = 24000\n",
    "    df_pause_between_sentences_tmp = df_lab_attached.query(\"audiobook_id == @audiobook_id and chapter_id_int == @chapter_id_int\")\n",
    "    df_pause_between_sentences_tmp = df_pause_between_sentences_tmp.reset_index(drop=True)\n",
    "    for s, e, l in df_pause_between_sentences_tmp[\"rule_based_detected_pause_ranges_in_sentence_-30_0.1\"].values[0]:\n",
    "        if 0.22 < l/sr < 0.28:\n",
    "            print(f\"{s/sr:.2f} - {e/sr:.2f} ({l/sr:.2f}) *\")\n",
    "        else:\n",
    "            print(f\"{s/sr:.2f} - {e/sr:.2f} ({l/sr:.2f})\")\n",
    "    \n",
    "    df_temp = read_lab(lab_path)[\"start end phoneme\".split()]\n",
    "    # display(df_temp)\n",
    "    sample_rate = 24000\n",
    "    db_threshold = -30\n",
    "    time_threshold = 100 / 1000  # 100ms\n",
    "    plot_all2(df_temp, wav_path, sample_rate, db_threshold, time_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"rule_based_detected_pause_ranges_in_sentence\" + threshold_str\n",
    "aid = \"audiobook_25\"\n",
    "df_aid = df_pause_between_sentences.query(\"audiobook_id == @aid\")\n",
    "\n",
    "mask = (df_aid[col] > 0.220) & (df_aid[col] < 0.280)\n",
    "\n",
    "df_aid[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_from_aid_chap(\"audiobook_25\", 44, df_lab_attached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_from_aid_chap(\"audiobook_25\", 10, df_lab_attached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_from_aid_chap(\"audiobook_25\", 51, df_lab_attached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_from_aid_chap(\"audiobook_25\", 113, df_lab_attached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"rule_based_detected_pause_ranges_in_sentence\" + threshold_str\n",
    "aid = \"audiobook_40\"\n",
    "df_aid = df_pause_between_sentences.query(\"audiobook_id == @aid\")\n",
    "\n",
    "mask = (df_aid[col] > 0.220) & (df_aid[col] < 0.280)\n",
    "\n",
    "df_aid[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_from_aid_chap(\"audiobook_40\", 6, df_lab_attached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_from_aid_chap(\"audiobook_40\", 227, df_lab_attached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_from_aid_chap(\"audiobook_40\", 257, df_lab_attached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"rule_based_detected_pause_ranges_in_sentence\" + threshold_str\n",
    "aid = \"audiobook_13\"\n",
    "df_aid = df_pause_between_sentences.query(\"audiobook_id == @aid\")\n",
    "\n",
    "mask = (df_aid[col] > 0.220) & (df_aid[col] < 0.280)\n",
    "\n",
    "df_aid[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_from_aid_chap(\"audiobook_13\", 12, df_lab_attached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_from_aid_chap(\"audiobook_13\", 70, df_lab_attached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"rule_based_detected_pause_ranges_in_sentence\" + threshold_str\n",
    "aid = \"audiobook_42\"\n",
    "df_aid = df_pause_between_sentences.query(\"audiobook_id == @aid\")\n",
    "\n",
    "mask = (df_aid[col] > 0.220) & (df_aid[col] < 0.280)\n",
    "\n",
    "df_aid[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_from_aid_chap(\"audiobook_42\", 78, df_lab_attached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_from_aid_chap(\"audiobook_42\", 134, df_lab_attached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"rule_based_detected_pause_ranges_in_sentence\" + threshold_str\n",
    "aid = \"audiobook_26\"\n",
    "df_aid = df_pause_between_sentences.query(\"audiobook_id == @aid\")\n",
    "\n",
    "mask = (df_aid[col] > 0.220) & (df_aid[col] < 0.280)\n",
    "\n",
    "df_aid[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_from_aid_chap(\"audiobook_26\", 10, df_lab_attached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_from_aid_chap(\"audiobook_26\", 11, df_lab_attached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"rule_based_detected_pause_ranges_in_sentence\" + threshold_str\n",
    "aid = \"audiobook_56\"\n",
    "df_aid = df_pause_between_sentences.query(\"audiobook_id == @aid\")\n",
    "\n",
    "mask = (df_aid[col] > 0.220) & (df_aid[col] < 0.280)\n",
    "\n",
    "df_aid[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"rule_based_detected_pause_ranges_in_sentence\" + threshold_str\n",
    "aid = \"audiobook_69\"\n",
    "df_aid = df_pause_between_sentences.query(\"audiobook_id == @aid\")\n",
    "\n",
    "mask = (df_aid[col] > 0.220) & (df_aid[col] < 0.280)\n",
    "\n",
    "df_aid[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_from_aid_chap(\"audiobook_69\", 14, df_lab_attached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_from_aid_chap(\"audiobook_69\", 69, df_lab_attached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_from_aid_chap(\"audiobook_69\", 81, df_lab_attached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"rule_based_detected_pause_ranges_in_sentence\" + threshold_str\n",
    "aid = \"audiobook_34\"\n",
    "df_aid = df_pause_between_sentences.query(\"audiobook_id == @aid\")\n",
    "\n",
    "mask = (df_aid[col] > 0.220) & (df_aid[col] < 0.280)\n",
    "\n",
    "df_aid[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_from_aid_chap(\"audiobook_34\", 2, df_lab_attached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_from_aid_chap(\"audiobook_34\", 8, df_lab_attached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_from_aid_chap(\"audiobook_34\",16, df_lab_attached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jvs_data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
