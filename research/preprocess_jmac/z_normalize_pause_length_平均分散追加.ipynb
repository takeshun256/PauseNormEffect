{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT学習用に作成された形態素/PAUSE/形態素のリストを読み込み、PAUSEの長さを正規化する\n",
    "\n",
    "ポーズ長の正規化（z_normalize_pause_length.ipynb）\n",
    "- (それぞれの値と、正規化後の分布を可視化): 優先度中\n",
    "- 全体\n",
    "- オーディオブック（=話者x文章作品）ごと\n",
    "- 地の文か否か\n",
    "- オーディオブックごと x 地の文か否か\n",
    "- 話者ごと\n",
    "- 文章作品ごと(あれば)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/takeshun256/PausePrediction\")\n",
    "\n",
    "# import own library\n",
    "from config import DATA_DIR, DATA_TAKESHUN256_DIR, SRC_DIR, DATA_IN_ROOT_DIR\n",
    "\n",
    "# define path\n",
    "corpus_name = \"jmac\"\n",
    "exp_name = \"03_VAD_Adjusted\"\n",
    "exp_dir = Path(DATA_TAKESHUN256_DIR) / corpus_name / exp_name\n",
    "audiobook_yaml_path = Path(DATA_IN_ROOT_DIR) / corpus_name / \"text_audio_dict_new.yaml\"\n",
    "speaker_csv_path = \"/home/takeshun256/PausePrediction/data_pub/jmac/bookdata-speaker.csv\"\n",
    "speaker_gender_csv_path =  \"/home/takeshun256/PausePrediction/data_pub/jmac/speaker_gender.csv\"\n",
    "\n",
    "assert exp_dir.exists()\n",
    "assert audiobook_yaml_path.exists()\n",
    "\n",
    "# audio book data\n",
    "with open(audiobook_yaml_path, \"rb\") as f:\n",
    "    audiobook_dict = yaml.safe_load(f)\n",
    "\n",
    "# speaker data\n",
    "df_speaker = pd.read_csv(speaker_csv_path)\n",
    "df_gender = pd.read_csv(speaker_gender_csv_path)\n",
    "\n",
    "\n",
    "# 80ms\n",
    "df_train_80ms = pd.read_pickle(exp_dir / \"bert_traindata_pause_position_with_length_wo_sokuon_80ms.pkl\")\n",
    "df_train_80ms.drop(columns=[\"morp_pause_str\", \"morp_pause_clip\"], inplace=True)\n",
    "\n",
    "# 100ms\n",
    "df_train_100ms = pd.read_pickle(exp_dir / \"bert_traindata_pause_position_with_length_wo_sokuon_100ms.pkl\")\n",
    "df_train_100ms.drop(columns=[\"morp_pause_str\", \"morp_pause_clip\"], inplace=True)\n",
    "\n",
    "print(\"audio book data\")\n",
    "print(len(audiobook_dict))\n",
    "pprint(audiobook_dict[list(audiobook_dict.keys())[0]])\n",
    "print(\"speaker data\")\n",
    "display(df_speaker.head())\n",
    "print(\"speaker gender data\")\n",
    "display(df_gender.head())\n",
    "print(\"80ms\")\n",
    "print(df_train_80ms.shape)\n",
    "print(\"100ms\")\n",
    "print(df_train_100ms.shape)\n",
    "display(df_train_80ms.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT学習データに、発話ごとに地の文かどうかのラベルを付与する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 発話ごとに地の文かどうかのラベルを付与する\n",
    "- オーディオブックごとに、話者情報, 本の情報を付与する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audiobook_dictからキャラクター情報を抽出し、新しいラベル列を作成する関数\n",
    "\n",
    "# ============== 地の文かどうか ==============\n",
    "# Narration: 1, Others: 0\n",
    "def extract_narrative_label(audiobook_data: dict):\n",
    "    labels = []\n",
    "    for audio_name, audio_info in audiobook_data.items():\n",
    "        for chapter_n, chapter_info in enumerate(audio_info[\"text\"]):\n",
    "            # 0埋め3桁の章番号\n",
    "            chapter_name = f\"{chapter_n:03}\"\n",
    "            if chapter_info['character'] == 'narrative':\n",
    "                labels.append([audio_name, chapter_name, 1])\n",
    "            else:\n",
    "                labels.append([audio_name, chapter_name, 0])\n",
    "    labels = pd.DataFrame(labels, columns=[\"audiobook_name\", \"chapter_name\", \"is_narrative\"])\n",
    "    return labels\n",
    "\n",
    "narrative_label_df = extract_narrative_label(audiobook_dict)\n",
    "display(narrative_label_df.head())\n",
    "# 80ms\n",
    "df_train_80ms_labeled = pd.merge(df_train_80ms, narrative_label_df, on=[\"audiobook_name\", \"chapter_name\"], how=\"left\")\n",
    "# 100ms\n",
    "df_train_100ms_labeled = pd.merge(df_train_100ms, narrative_label_df, on=[\"audiobook_name\", \"chapter_name\"], how=\"left\")\n",
    "\n",
    "\n",
    "# ============== 話者 ==============\n",
    "def extract_speaker_label(audiobook_data: dict, speaker_df: pd.DataFrame):\n",
    "    # wavファイル名をキーにして、話者名を取得 (mp3は分割されているため、wavファイル名の方で結合する)\n",
    "    audio_names = []\n",
    "    wavs = []\n",
    "    for audio_name, audio_info in audiobook_data.items():\n",
    "        audio_names.append(audio_name)\n",
    "        wavs.append(Path(audio_info[\"wav\"]).name)\n",
    "    df_audio = pd.DataFrame({\"audiobook_name\": audio_names, \"wav\": wavs})\n",
    "    df_speaker_one = speaker_df.copy()\n",
    "    df_speaker_one[\"speaker\"] = df_speaker_one[\"speaker\"].apply(lambda x: x.split(\",\")[0]) # 複数の話者がいる場合、最初の話者を取得\n",
    "    df_speaker_one = df_speaker_one[[\"speaker\", \"wav\",\"book\"]]\n",
    "    display(df_speaker_one.head())\n",
    "    before = len(df_audio)\n",
    "    df_audio_speaker = pd.merge(df_audio, df_speaker_one, on=\"wav\", how=\"inner\")\n",
    "    after = len(df_audio_speaker)\n",
    "    assert before == after\n",
    "    df_audio_speaker.drop(columns=[\"wav\"], inplace=True)\n",
    "    return df_audio_speaker\n",
    "\n",
    "speaker_label_df = extract_speaker_label(audiobook_dict, df_speaker)\n",
    "display(speaker_label_df.head())\n",
    "# 80ms\n",
    "df_train_80ms_labeled = pd.merge(df_train_80ms_labeled, speaker_label_df, on=[\"audiobook_name\"], how=\"left\")\n",
    "# 100ms\n",
    "df_train_100ms_labeled = pd.merge(df_train_100ms_labeled, speaker_label_df, on=[\"audiobook_name\"], how=\"left\")\n",
    "\n",
    "# ============== check ==============\n",
    "\n",
    "\n",
    "print(\"80ms\")\n",
    "print(df_train_80ms_labeled.shape)\n",
    "print(f\"isna: {df_train_80ms_labeled['is_narrative'].isna().sum()}\")\n",
    "print(\"100ms\")\n",
    "print(df_train_100ms_labeled.shape)\n",
    "print(f\"isna: {df_train_100ms_labeled['is_narrative'].isna().sum()}\")\n",
    "display(df_train_80ms_labeled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audibookのunique数\n",
    "print(\"audiobook_name\")\n",
    "print(df_train_80ms_labeled[\"audiobook_name\"].nunique())\n",
    "print(df_train_100ms_labeled[\"audiobook_name\"].nunique())\n",
    "\n",
    "# speakerのunique数\n",
    "print(\"speaker\")\n",
    "print(df_train_80ms_labeled[\"speaker\"].nunique())\n",
    "print(df_train_100ms_labeled[\"speaker\"].nunique())\n",
    "\n",
    "# bookのunique数\n",
    "print(\"book\")\n",
    "print(df_train_80ms_labeled[\"book\"].nunique())\n",
    "print(df_train_100ms_labeled[\"book\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章例\n",
    "print(df_train_80ms_labeled[\"morp_pause_clip_no_pause\"].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ポーズ長の外れ値を除去する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_outlier(sentence_elements: list):\n",
    "    if len(sentence_elements) == 0:\n",
    "        return sentence_elements\n",
    "    no_pause = \"[NO_PAUSE]\"\n",
    "    f = lambda x: float(x.split()[1][:-1]) if x.startswith(\"[PAUSE\") else 0\n",
    "    # 文間ポーズの長さが0未満, 15秒以上のものを削除\n",
    "    sentence_elements[0] = no_pause if f(sentence_elements[0]) < 0 or f(sentence_elements[0]) > 15 else sentence_elements[0]\n",
    "    sentence_elements[-1] = no_pause if f(sentence_elements[-1]) < 0 or f(sentence_elements[-1]) > 15 else sentence_elements[-1]\n",
    "    \n",
    "    # 文中ポーズの長さが0未満, 3秒以上のものを削除\n",
    "    for i in range(1, len(sentence_elements) - 1):\n",
    "        sentence_elements[i] = no_pause if f(sentence_elements[i]) < 0 or f(sentence_elements[i]) > 3 else sentence_elements[i]\n",
    "    return sentence_elements\n",
    "\n",
    "# サンプル文章からポーズの長さを抽出\n",
    "sample_sentence = [\n",
    "    '[PAUSE -0.1]', 'これ', '[NO_PAUSE]', 'は', '[PAUSE -0.1]', '、', '[PAUSE 5.0]', '私', '[NO_PAUSE]', 'が', \n",
    "    '[NO_PAUSE]', '小さい', '[NO_PAUSE]', 'とき', '[NO_PAUSE]', 'に', '[PAUSE 0.6171666666666666]', '、', '[NO_PAUSE]', \n",
    "    '村', '[NO_PAUSE]', 'の', '[NO_PAUSE]', '茂平', '[NO_PAUSE]', 'と', '[NO_PAUSE]', 'いう', '[NO_PAUSE]', 'おじいさん', \n",
    "    '[NO_PAUSE]', 'から', '[NO_PAUSE]', 'きい', '[NO_PAUSE]', 'た', '[NO_PAUSE]', 'お話', '[NO_PAUSE]', 'です', \n",
    "    '[PAUSE 18.0]'\n",
    "]\n",
    "\n",
    "print(delete_outlier(sample_sentence))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80ms\n",
    "df_train_80ms_labeled[\"morp_pause_clip_no_pause\"] = df_train_80ms_labeled[\"morp_pause_clip_no_pause\"].apply(lambda x: delete_outlier(x))\n",
    "\n",
    "# 100ms\n",
    "df_train_100ms_labeled[\"morp_pause_clip_no_pause\"] = df_train_100ms_labeled[\"morp_pause_clip_no_pause\"].apply(lambda x: delete_outlier(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ポーズ長の正規化：全体、オーディオブックごと、地の文か否か、オーディオブックごと x 地の文か否か"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文中のポーズの長さを抽出する関数\n",
    "def extract_pause_lengths_in_sentence(sentence_elements):\n",
    "    \"\"\"文中のポーズの長さを抽出する関数\"\"\"\n",
    "    # 文間ポーズを削除\n",
    "    sentence_elements = sentence_elements[1:-1]\n",
    "    \n",
    "    # [PAUSE 0.435]のような要素の数値を抽出\n",
    "    pause_lengths = []\n",
    "    for element in sentence_elements:\n",
    "        if element.startswith(\"[PAUSE\"):\n",
    "            pause_lengths.append(float(element.split()[1][:-1]))\n",
    "    return pause_lengths\n",
    "\n",
    "# サンプル文章からポーズの長さを抽出\n",
    "sample_sentence = [\n",
    "    '[PAUSE 0.435]', 'これ', '[NO_PAUSE]', 'は', '[PAUSE 0.6785]', '、', '[PAUSE 0.6785]', '私', '[NO_PAUSE]', 'が', \n",
    "    '[NO_PAUSE]', '小さい', '[NO_PAUSE]', 'とき', '[NO_PAUSE]', 'に', '[PAUSE 0.6171666666666666]', '、', '[NO_PAUSE]', \n",
    "    '村', '[NO_PAUSE]', 'の', '[NO_PAUSE]', '茂平', '[NO_PAUSE]', 'と', '[NO_PAUSE]', 'いう', '[NO_PAUSE]', 'おじいさん', \n",
    "    '[NO_PAUSE]', 'から', '[NO_PAUSE]', 'きい', '[NO_PAUSE]', 'た', '[NO_PAUSE]', 'お話', '[NO_PAUSE]', 'です', \n",
    "    '[PAUSE 0.2939583333333333]'\n",
    "]\n",
    "\n",
    "pause_lengths = extract_pause_lengths_in_sentence(sample_sentence)\n",
    "print(pause_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力されたdf全体のポーズ長の平均と分散を計算する関数\n",
    "def calc_mean_var_pause_length(df, pause_length_column=\"morp_pause_clip_no_pause\"):\n",
    "    \"\"\"入力されたdf全体のポーズ長の平均と分散を計算する関数\"\"\"\n",
    "    pause_lengths = []\n",
    "    for i in range(len(df)):\n",
    "        pause_lengths.extend(extract_pause_lengths_in_sentence(df[pause_length_column].values[i]))\n",
    "\n",
    "    pause_lengths = np.array(pause_lengths)\n",
    "    return pause_lengths.mean(), pause_lengths.var()\n",
    "\n",
    "# 80ms, audiobook_0のポーズ長の平均と分散\n",
    "print(\"80ms\")\n",
    "print(calc_mean_var_pause_length(df_train_80ms_labeled[df_train_80ms_labeled[\"audiobook_name\"] == \"audiobook_0\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80ms, 全体のポーズ長の平均と分散\n",
    "print(\"80ms all\")\n",
    "print(calc_mean_var_pause_length(df_train_80ms_labeled))\n",
    "\n",
    "# 100ms, 全体のポーズ長の平均と分散\n",
    "print(\"100ms all\")\n",
    "print(calc_mean_var_pause_length(df_train_100ms_labeled))\n",
    "\n",
    "# 80ms, オーディオブックごとのポーズ長の平均と分散\n",
    "print(\"80ms audiobook\")\n",
    "df_80ms_audiobook_mean_var = []\n",
    "for audibook_name in df_train_80ms_labeled[\"audiobook_name\"].unique():\n",
    "    df_80ms_audiobook_mean_var.append(\n",
    "        [audibook_name, *calc_mean_var_pause_length(df_train_80ms_labeled[df_train_80ms_labeled[\"audiobook_name\"] == audibook_name])]\n",
    "    )\n",
    "df_80ms_audiobook_mean_var = pd.DataFrame(df_80ms_audiobook_mean_var, columns=[\"audiobook_name\", \"mean\", \"var\"])\n",
    "display(df_80ms_audiobook_mean_var.head())\n",
    "\n",
    "# 100ms, オーディオブックごとのポーズ長の平均と分散\n",
    "print(\"100ms audiobook\")\n",
    "df_100ms_audiobook_mean_var = []\n",
    "for audibook_name in df_train_100ms_labeled[\"audiobook_name\"].unique():\n",
    "    df_100ms_audiobook_mean_var.append(\n",
    "        [audibook_name, *calc_mean_var_pause_length(df_train_100ms_labeled[df_train_100ms_labeled[\"audiobook_name\"] == audibook_name])]\n",
    "    )\n",
    "df_100ms_audiobook_mean_var = pd.DataFrame(df_100ms_audiobook_mean_var, columns=[\"audiobook_name\", \"mean\", \"var\"])\n",
    "display(df_100ms_audiobook_mean_var.head())\n",
    "\n",
    "# 80ms, 地の文か否かでポーズ長の平均と分散を比較\n",
    "print(\"80ms narrative\")\n",
    "print(calc_mean_var_pause_length(df_train_80ms_labeled[df_train_80ms_labeled[\"is_narrative\"] == 1]))\n",
    "print(\"80ms non-narrative\")\n",
    "print(calc_mean_var_pause_length(df_train_80ms_labeled[df_train_80ms_labeled[\"is_narrative\"] == 0]))\n",
    "\n",
    "# 100ms, 地の文か否かでポーズ長の平均と分散を比較\n",
    "print(\"100ms narrative\")\n",
    "print(calc_mean_var_pause_length(df_train_100ms_labeled[df_train_100ms_labeled[\"is_narrative\"] == 1]))\n",
    "print(\"100ms non-narrative\")\n",
    "print(calc_mean_var_pause_length(df_train_100ms_labeled[df_train_100ms_labeled[\"is_narrative\"] == 0]))\n",
    "\n",
    "# 80ms, オーディオブックごとに地の文か否かでポーズ長の平均と分散を比較\n",
    "print(\"80ms audiobook narrative vs non-narrative\")\n",
    "df_80ms_audiobook_mean_var_narrative = []\n",
    "df_80ms_audiobook_mean_var_non_narrative = []\n",
    "for audibook_name in df_train_80ms_labeled[\"audiobook_name\"].unique():\n",
    "    df_80ms_audiobook_mean_var_narrative.append(\n",
    "        [audibook_name, *calc_mean_var_pause_length(df_train_80ms_labeled[(df_train_80ms_labeled[\"audiobook_name\"] == audibook_name) & (df_train_80ms_labeled[\"is_narrative\"] == 1)])]\n",
    "    )\n",
    "    df_80ms_audiobook_mean_var_non_narrative.append(\n",
    "        [audibook_name, *calc_mean_var_pause_length(df_train_80ms_labeled[(df_train_80ms_labeled[\"audiobook_name\"] == audibook_name) & (df_train_80ms_labeled[\"is_narrative\"] == 0)])]\n",
    "    )\n",
    "df_80ms_audiobook_mean_var_narrative = pd.DataFrame(df_80ms_audiobook_mean_var_narrative, columns=[\"audiobook_name\", \"mean\", \"var\"])\n",
    "df_80ms_audiobook_mean_var_non_narrative = pd.DataFrame(df_80ms_audiobook_mean_var_non_narrative, columns=[\"audiobook_name\", \"mean\", \"var\"])\n",
    "display(df_80ms_audiobook_mean_var_narrative.head())\n",
    "display(df_80ms_audiobook_mean_var_non_narrative.head())\n",
    "\n",
    "# 100ms, オーディオブックごとに地の文か否かでポーズ長の平均と分散を比較\n",
    "print(\"100ms audiobook narrative vs non-narrative\")\n",
    "df_100ms_audiobook_mean_var_narrative = []\n",
    "df_100ms_audiobook_mean_var_non_narrative = []\n",
    "for audibook_name in df_train_100ms_labeled[\"audiobook_name\"].unique():\n",
    "    df_100ms_audiobook_mean_var_narrative.append(\n",
    "        [audibook_name, *calc_mean_var_pause_length(df_train_100ms_labeled[(df_train_100ms_labeled[\"audiobook_name\"] == audibook_name) & (df_train_100ms_labeled[\"is_narrative\"] == 1)])]\n",
    "    )\n",
    "    df_100ms_audiobook_mean_var_non_narrative.append(\n",
    "        [audibook_name, *calc_mean_var_pause_length(df_train_100ms_labeled[(df_train_100ms_labeled[\"audiobook_name\"] == audibook_name) & (df_train_100ms_labeled[\"is_narrative\"] == 0)])]\n",
    "    )\n",
    "df_100ms_audiobook_mean_var_narrative = pd.DataFrame(df_100ms_audiobook_mean_var_narrative, columns=[\"audiobook_name\", \"mean\", \"var\"])\n",
    "df_100ms_audiobook_mean_var_non_narrative = pd.DataFrame(df_100ms_audiobook_mean_var_non_narrative, columns=[\"audiobook_name\", \"mean\", \"var\"])\n",
    "display(df_100ms_audiobook_mean_var_narrative.head())\n",
    "display(df_100ms_audiobook_mean_var_non_narrative.head())\n",
    "\n",
    "# 80ms, 話者ごとのポーズ長の平均と分散を比較\n",
    "print(\"80ms speaker\")\n",
    "df_80ms_speaker_mean_var = []\n",
    "for speaker in df_train_80ms_labeled[\"speaker\"].unique():\n",
    "    df_80ms_speaker_mean_var.append(\n",
    "        [speaker, *calc_mean_var_pause_length(df_train_80ms_labeled[df_train_80ms_labeled[\"speaker\"] == speaker])]\n",
    "    )\n",
    "df_80ms_speaker_mean_var = pd.DataFrame(df_80ms_speaker_mean_var, columns=[\"speaker\", \"mean\", \"var\"])\n",
    "display(df_80ms_speaker_mean_var.head())\n",
    "\n",
    "# 100ms, 話者ごとのポーズ長の平均と分散を比較\n",
    "print(\"100ms speaker\")\n",
    "df_100ms_speaker_mean_var = []\n",
    "for speaker in df_train_100ms_labeled[\"speaker\"].unique():\n",
    "    df_100ms_speaker_mean_var.append(\n",
    "        [speaker, *calc_mean_var_pause_length(df_train_100ms_labeled[df_train_100ms_labeled[\"speaker\"] == speaker])]\n",
    "    )\n",
    "df_100ms_speaker_mean_var = pd.DataFrame(df_100ms_speaker_mean_var, columns=[\"speaker\", \"mean\", \"var\"])\n",
    "display(df_100ms_speaker_mean_var.head())\n",
    "\n",
    "# 80ms, 文章作品ごとのポーズ長の平均と分散を比較\n",
    "print(\"80ms book\")\n",
    "df_80ms_book_mean_var = []\n",
    "for book in df_train_80ms_labeled[\"book\"].unique():\n",
    "    df_80ms_book_mean_var.append(\n",
    "        [book, *calc_mean_var_pause_length(df_train_80ms_labeled[df_train_80ms_labeled[\"book\"] == book])]\n",
    "    )\n",
    "df_80ms_book_mean_var = pd.DataFrame(df_80ms_book_mean_var, columns=[\"book\", \"mean\", \"var\"])\n",
    "display(df_80ms_book_mean_var.head())\n",
    "\n",
    "# 100ms, 文章作品ごとのポーズ長の平均と分散を比較\n",
    "print(\"100ms book\")\n",
    "df_100ms_book_mean_var = []\n",
    "for book in df_train_100ms_labeled[\"book\"].unique():\n",
    "    df_100ms_book_mean_var.append(\n",
    "        [book, *calc_mean_var_pause_length(df_train_100ms_labeled[df_train_100ms_labeled[\"book\"] == book])]\n",
    "    )\n",
    "df_100ms_book_mean_var = pd.DataFrame(df_100ms_book_mean_var, columns=[\"book\", \"mean\", \"var\"])\n",
    "display(df_100ms_book_mean_var.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各ポーズ長について、与えられた平均と分散をもとに正規化する関数\n",
    "def normalize_pause_length(pause_length: float, mean: float, var: float) -> float:\n",
    "    \"\"\"各ポーズ長について、与えられた平均と分散をもとに正規化する関数\"\"\"\n",
    "    assert var > 0, \"var must be positive\"\n",
    "    assert pause_length >= 0, \"pause_length must be positive\"\n",
    "    assert mean >= 0, \"mean must be positive\"\n",
    "    return (pause_length - mean) / np.sqrt(var)\n",
    "\n",
    "# [PAUSE 0.4352]のポーズ長を正規化する関数\n",
    "def normalize_pause_length_in_text(text: str, mean: float, var: float) -> str:\n",
    "    \"\"\"[PAUSE 0.4352]のポーズ長を正規化する関数\"\"\"\n",
    "    if text.startswith(\"[PAUSE\"):\n",
    "        pause_length = float(text.split()[1][:-1])\n",
    "        normalized_pause_length = normalize_pause_length(float(pause_length), mean, var)\n",
    "        return f\"[PAUSE {normalized_pause_length:.4f}]\"\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# PAUSE入りのテキストリスト内の文中ポーズ長を正規化する関数\n",
    "def normalize_pause_lengths_in_texts(texts: list, mean: float, var: float) -> list:\n",
    "    \"\"\"PAUSE入りのテキストリスト内の文中ポーズ長を正規化する関数\"\"\"\n",
    "    if len(texts) == 0:\n",
    "        return texts\n",
    "    normalized_texts = []\n",
    "    # 文間ポーズはそのまま\n",
    "    normalized_texts.append(texts[0])\n",
    "    for text in texts[1:-1]:\n",
    "        normalized_texts.append(normalize_pause_length_in_text(text, mean, var))\n",
    "    normalized_texts.append(texts[-1])\n",
    "    return normalized_texts\n",
    "\n",
    "# サンプル文章のポーズ長を正規化\n",
    "sample_sentence = [\n",
    "    '[PAUSE 0.435]', 'これ', '[NO_PAUSE]', 'は', '[PAUSE 0.6785]', '、', '[PAUSE 0.6785]', '私', '[NO_PAUSE]', 'が', \n",
    "    '[NO_PAUSE]', '小さい', '[NO_PAUSE]', 'とき', '[NO_PAUSE]', 'に', '[PAUSE 0.6171666666666666]', '、', '[NO_PAUSE]', \n",
    "    '村', '[NO_PAUSE]', 'の', '[NO_PAUSE]', '茂平', '[NO_PAUSE]', 'と', '[NO_PAUSE]', 'いう', '[NO_PAUSE]', 'おじいさん', \n",
    "    '[NO_PAUSE]', 'から', '[NO_PAUSE]', 'きい', '[NO_PAUSE]', 'た', '[NO_PAUSE]', 'お話', '[NO_PAUSE]', 'です', \n",
    "    '[PAUSE 0.2939583333333333]'\n",
    "]\n",
    "sample_mean = 0.5\n",
    "sample_var = 0.01\n",
    "\n",
    "print(normalize_pause_length(0.435, sample_mean, sample_var))\n",
    "print(normalize_pause_length_in_text(sample_sentence[0], sample_mean, sample_var))\n",
    "pprint(normalize_pause_lengths_in_texts(sample_sentence, sample_mean, sample_var))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グループを指定して正規化する関数\n",
    "def normalize_pause_length_with_group(df: pd.DataFrame, group: str, added_col: str) -> pd.DataFrame:\n",
    "    \"\"\"グループを指定して正規化する関数\"\"\"\n",
    "    df_normalized = df.copy()\n",
    "    # 複数グループ指定に対応していないため、コメントアウトして、groupbyで置き換え\n",
    "    # df_mean_var = []\n",
    "    # for group_name in df[group].unique():\n",
    "    #     df_mean_var.append(\n",
    "    #         [group_name, *calc_mean_var_pause_length(df[df[group] == group_name])]\n",
    "    #     )\n",
    "    # df_mean_var = pd.DataFrame(df_mean_var, columns=[group, \"mean\", \"var\"])\n",
    "    if group == \"\":\n",
    "        mean, var = calc_mean_var_pause_length(df)\n",
    "        df_normalized[\"mean\"] = mean\n",
    "        df_normalized[\"var\"] = var\n",
    "    else:\n",
    "        df_mean_var = df.groupby(group).apply(lambda x: pd.Series([*calc_mean_var_pause_length(x)], index=[\"mean\", \"var\"])).reset_index()\n",
    "        df_normalized = pd.merge(df_normalized, df_mean_var, on=group, how=\"left\")\n",
    "    df_normalized[added_col] = df_normalized.apply(lambda x: normalize_pause_lengths_in_texts(x[\"morp_pause_clip_no_pause\"], x[\"mean\"], x[\"var\"]), axis=1)\n",
    "    # df_normalized = df_normalized.drop([\"mean\", \"var\"], axis=1)\n",
    "    # mean, varをそれぞれrenameする\n",
    "    rename_dict = {\"\": \"all\", \"audiobook_name\": \"audiobook\", \"is_narrative\": \"narrative\", \"audiobook_name_is_narrative\": \"audiobook_narrative\", \"speaker\": \"speaker\", \"book\": \"book\"}\n",
    "    rename_group = rename_dict[group] if type(group) == str else \"_\".join([rename_dict[g] for g in group])\n",
    "    df_normalized.rename(columns={\"mean\": f\"mean_{rename_group}\", \"var\": f\"var_{rename_group}\"}, inplace=True)\n",
    "    return df_normalized\n",
    "\n",
    "# 80ms, オーディオブックごとに正規化\n",
    "display(df_train_80ms_labeled.head())\n",
    "display(normalize_pause_length_with_group(df_train_80ms_labeled, \"audiobook_name\", \"morp_pause_clip_no_pause_normalized\").head())\n",
    "\n",
    "# 80ms, オーディオブックごとに、地の文か否かで正規化\n",
    "display(normalize_pause_length_with_group(df_train_80ms_labeled, \"audiobook_name is_narrative\".split(), \"morp_pause_clip_no_pause_normalized\").head())\n",
    "\n",
    "# 80ms, 話者ごとに正規化\n",
    "display(normalize_pause_length_with_group(df_train_80ms_labeled, \"speaker\", \"morp_pause_clip_no_pause_normalized\").head())\n",
    "\n",
    "# 80ms, 文章作品ごとに正規化\n",
    "display(normalize_pause_length_with_group(df_train_80ms_labeled, \"book\", \"morp_pause_clip_no_pause_normalized\").head())\n",
    "\n",
    "# 80ms 全体で正規化\n",
    "df_train_80ms_labeled_normalized = normalize_pause_length_with_group(df_train_80ms_labeled, \"\", \"morp_pause_clip_no_pause_normalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 平均と分散を可視化, それぞれの値と、正規化後の分布を可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80ms 全体の正規化前後のポーズ長の平均と分散を分布上にプロット\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.suptitle(\"80ms pauses normalized by all\", fontsize=20)\n",
    "plt.subplot(121)\n",
    "plt.title(\"before normalization\")\n",
    "plt.xlabel(\"pause length\")\n",
    "plt.ylabel(\"frequency\") \n",
    "plt.hist(df_train_80ms_labeled[\"morp_pause_clip_no_pause\"].apply(extract_pause_lengths_in_sentence).explode().astype(float), bins=100)\n",
    "# 平均値と分散を表示\n",
    "mean, var = calc_mean_var_pause_length(df_train_80ms_labeled)\n",
    "plt.axvline(mean, color=\"red\", label=f\"mean = {mean:.4f}\")\n",
    "plt.axvline(mean + np.sqrt(var), color=\"green\", label=f\"mean + sqrt(var) = {mean + np.sqrt(var):.4f}\")\n",
    "plt.axvline(mean - np.sqrt(var), color=\"green\", label=f\"mean - sqrt(var) = {mean - np.sqrt(var):.4f}\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"after normalization\")\n",
    "plt.xlabel(\"pause length\")\n",
    "plt.ylabel(\"frequency\")\n",
    "df_train_80ms_labeled_normalized = normalize_pause_length_with_group(df_train_80ms_labeled, \"\", \"morp_pause_clip_no_pause_normalized\")\n",
    "plt.hist(df_train_80ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float), bins=100)\n",
    "mean, var = calc_mean_var_pause_length(df_train_80ms_labeled_normalized, \"morp_pause_clip_no_pause_normalized\")\n",
    "plt.axvline(mean, color=\"red\", label=f\"mean = {mean:.4f}\")\n",
    "plt.axvline(mean + np.sqrt(var), color=\"green\", label=f\"mean + sqrt(var) = {mean + np.sqrt(var):.4f}\")\n",
    "plt.axvline(mean - np.sqrt(var), color=\"green\", label=f\"mean - sqrt(var) = {mean - np.sqrt(var):.4f}\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100ms 全体の正規化前後のポーズ長の平均と分散を分布上にプロット\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.suptitle(\"100ms pauses normalized by all\", fontsize=20)\n",
    "plt.subplot(121)\n",
    "plt.title(\"before normalization\")\n",
    "plt.xlabel(\"pause length\")\n",
    "plt.ylabel(\"frequency\") \n",
    "plt.hist(df_train_100ms_labeled[\"morp_pause_clip_no_pause\"].apply(extract_pause_lengths_in_sentence).explode().astype(float), bins=100)\n",
    "# 平均値と分散を表示\n",
    "mean, var = calc_mean_var_pause_length(df_train_100ms_labeled)\n",
    "plt.axvline(mean, color=\"red\", label=f\"mean = {mean:.4f}\")\n",
    "plt.axvline(mean + np.sqrt(var), color=\"green\", label=f\"mean + sqrt(var) = {mean + np.sqrt(var):.4f}\")\n",
    "plt.axvline(mean - np.sqrt(var), color=\"green\", label=f\"mean - sqrt(var) = {mean - np.sqrt(var):.4f}\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"after normalization\")\n",
    "plt.xlabel(\"pause length\")\n",
    "plt.ylabel(\"frequency\")\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled, \"\", \"morp_pause_clip_no_pause_normalized\")\n",
    "plt.hist(df_train_100ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float), bins=100)\n",
    "mean, var = calc_mean_var_pause_length(df_train_100ms_labeled_normalized, \"morp_pause_clip_no_pause_normalized\")\n",
    "plt.axvline(mean, color=\"red\", label=f\"mean = {mean:.4f}\")\n",
    "plt.axvline(mean + np.sqrt(var), color=\"green\", label=f\"mean + sqrt(var) = {mean + np.sqrt(var):.4f}\")\n",
    "plt.axvline(mean - np.sqrt(var), color=\"green\", label=f\"mean - sqrt(var) = {mean - np.sqrt(var):.4f}\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.suptitle(\"80ms pauses is_narrative vs not is_narrative\", fontsize=20)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.suptitle(\"100ms pauses normalized by all\", fontsize=20)\n",
    "plt.subplot(121)\n",
    "plt.title(\"before normalization\")\n",
    "plt.xlabel(\"pause length\")\n",
    "plt.ylabel(\"frequency\") \n",
    "# 地の文のポーズ長の分布\n",
    "narrative_pause_lengths = df_train_100ms_labeled[df_train_100ms_labeled[\"is_narrative\"] == 1][\"morp_pause_clip_no_pause\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "plt.hist(narrative_pause_lengths, bins=100, alpha=0.5, label='narative')\n",
    "\n",
    "# 非地の文のポーズ長の分布\n",
    "non_narrative_pause_lengths = df_train_100ms_labeled[df_train_100ms_labeled[\"is_narrative\"] == 0][\"morp_pause_clip_no_pause\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "plt.hist(non_narrative_pause_lengths, bins=100, alpha=0.5, label='non-narative')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"after normalization\")\n",
    "plt.xlabel(\"pause length\")\n",
    "plt.ylabel(\"frequency\")\n",
    "\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled, \"is_narrative\", \"morp_pause_clip_no_pause_normalized\")\n",
    "# 地の文のポーズ長の分布\n",
    "narrative_pause_lengths_normalized = df_train_100ms_labeled_normalized[df_train_100ms_labeled_normalized[\"is_narrative\"] == 1][\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "plt.hist(narrative_pause_lengths_normalized, bins=100, alpha=0.5, label='narative')\n",
    "\n",
    "# 非地の文のポーズ長の分布\n",
    "non_narrative_pause_lengths_normalized = df_train_100ms_labeled_normalized[df_train_100ms_labeled_normalized[\"is_narrative\"] == 0][\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "plt.hist(non_narrative_pause_lengths_normalized, bins=100, alpha=0.5, label='non-narative')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# オーディオブックごとのx=平均、y=分散の散布図をプロットする関数\n",
    "def plot_mean_var_scatter(df: pd.DataFrame, group: str, x_col: str, y_col: str):\n",
    "    \"\"\"オーディオブックごとのx=平均、y=分散の散布図をプロットする関数\"\"\"\n",
    "    df_mean_var = df.groupby(group).apply(lambda x: pd.Series([*calc_mean_var_pause_length(x)], index=[\"mean\", \"var\"])).reset_index()\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.title(f\"mean-var scatter plot by {group}\")\n",
    "    plt.xlabel(\"mean\")\n",
    "    plt.ylabel(\"var\")\n",
    "    plt.scatter(df_mean_var[x_col], df_mean_var[y_col])\n",
    "    \n",
    "    # y=xの直線を引く\n",
    "    plt.plot([0, 0.5], [0, 0.5], color=\"red\")\n",
    "    # for i in range(len(df_mean_var)):\n",
    "    #     plt.annotate(df_mean_var[group].values[i], (df_mean_var[x_col].values[i], df_mean_var[y_col].values[i]))\n",
    "    \n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.show()\n",
    "\n",
    "plot_mean_var_scatter(df_train_80ms_labeled, \"audiobook_name\", \"mean\", \"var\")\n",
    "plot_mean_var_scatter(df_train_80ms_labeled, \"is_narrative\", \"mean\", \"var\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80ms, 各グループごとの平均と分散の散布図を色分けしてプロット\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.suptitle(\"80ms mean-var scatter plot groupby\", fontsize=20)\n",
    "plt.subplot(111)\n",
    "plt.title(\"80ms groupby\")\n",
    "plt.xlabel(\"mean\")\n",
    "plt.ylabel(\"var\")\n",
    "plt.scatter(df_80ms_audiobook_mean_var[\"mean\"], df_80ms_audiobook_mean_var[\"var\"])\n",
    "mean, var = calc_mean_var_pause_length(df_train_80ms_labeled)\n",
    "plt.axvline(mean, color=\"red\", label=f\"all mean = {mean:.4f} var = {var:.4f}\")\n",
    "plt.axhline(var, color=\"red\")\n",
    "\n",
    "df_train_80ms_labeled_normalized = normalize_pause_length_with_group(df_train_80ms_labeled, \"audiobook_name\", \"morp_pause_clip_no_pause_normalized\")\n",
    "df_mean_var = df_train_80ms_labeled_normalized.groupby(\"audiobook_name\").apply(lambda x: pd.Series([*calc_mean_var_pause_length(x)], index=[\"mean\", \"var\"])).reset_index()\n",
    "plt.scatter(df_mean_var[\"mean\"], df_mean_var[\"var\"], label=\"groupby audiobook\", color=\"orange\")\n",
    "\n",
    "df_train_80ms_labeled_normalized = normalize_pause_length_with_group(df_train_80ms_labeled, \"is_narrative\", \"morp_pause_clip_no_pause_normalized\")\n",
    "df_mean_var = df_train_80ms_labeled_normalized.groupby(\"is_narrative\").apply(lambda x: pd.Series([*calc_mean_var_pause_length(x)], index=[\"mean\", \"var\"])).reset_index()\n",
    "plt.scatter(df_mean_var[\"mean\"], df_mean_var[\"var\"], label=\"groupby is_narrative\", color=\"green\")\n",
    "\n",
    "df_train_80ms_labeled_normalized = normalize_pause_length_with_group(df_train_80ms_labeled, \"audiobook_name is_narrative\".split(), \"morp_pause_clip_no_pause_normalized\")\n",
    "df_mean_var = df_train_80ms_labeled_normalized.groupby(\"audiobook_name is_narrative\".split()).apply(lambda x: pd.Series([*calc_mean_var_pause_length(x)], index=[\"mean\", \"var\"])).reset_index()\n",
    "plt.scatter(df_mean_var[\"mean\"], df_mean_var[\"var\"], label=\"groupby (audiobook, is_narrative)\", color=\"blue\")\n",
    "\n",
    "df_train_80ms_labeled_normalized = normalize_pause_length_with_group(df_train_80ms_labeled, \"speaker\", \"morp_pause_clip_no_pause_normalized\")\n",
    "df_mean_var = df_train_80ms_labeled_normalized.groupby(\"speaker\").apply(lambda x: pd.Series([*calc_mean_var_pause_length(x)], index=[\"mean\", \"var\"])).reset_index()\n",
    "plt.scatter(df_mean_var[\"mean\"], df_mean_var[\"var\"], label=\"groupby speaker\", color=\"purple\")\n",
    "\n",
    "df_train_80ms_labeled_normalized = normalize_pause_length_with_group(df_train_80ms_labeled, \"book\", \"morp_pause_clip_no_pause_normalized\")\n",
    "df_mean_var = df_train_80ms_labeled_normalized.groupby(\"book\").apply(lambda x: pd.Series([*calc_mean_var_pause_length(x)], index=[\"mean\", \"var\"])).reset_index()\n",
    "plt.scatter(df_mean_var[\"mean\"], df_mean_var[\"var\"], label=\"groupby book\", color=\"brown\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100ms, 各グループごとの平均と分散の散布図を色分けしてプロット\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.suptitle(\"100ms mean-var scatter plot groupby\", fontsize=20)\n",
    "plt.subplot(111)\n",
    "plt.title(\"100ms groupby\")\n",
    "plt.xlabel(\"mean\")\n",
    "plt.ylabel(\"var\")\n",
    "plt.scatter(df_100ms_audiobook_mean_var[\"mean\"], df_100ms_audiobook_mean_var[\"var\"])\n",
    "mean, var = calc_mean_var_pause_length(df_train_100ms_labeled)\n",
    "plt.axvline(mean, color=\"red\", label=f\"all mean = {mean:.4f} var = {var:.4f}\")\n",
    "plt.axhline(var, color=\"red\")\n",
    "\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled, \"audiobook_name\", \"morp_pause_clip_no_pause_normalized\")\n",
    "df_mean_var = df_train_100ms_labeled_normalized.groupby(\"audiobook_name\").apply(lambda x: pd.Series([*calc_mean_var_pause_length(x)], index=[\"mean\", \"var\"])).reset_index()\n",
    "plt.scatter(df_mean_var[\"mean\"], df_mean_var[\"var\"], label=\"groupby audiobook\", color=\"orange\")\n",
    "\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled, \"is_narrative\", \"morp_pause_clip_no_pause_normalized\")\n",
    "df_mean_var = df_train_100ms_labeled_normalized.groupby(\"is_narrative\").apply(lambda x: pd.Series([*calc_mean_var_pause_length(x)], index=[\"mean\", \"var\"])).reset_index()\n",
    "plt.scatter(df_mean_var[\"mean\"], df_mean_var[\"var\"], label=\"groupby is_narrative\", color=\"green\")\n",
    "\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled, \"audiobook_name is_narrative\".split(), \"morp_pause_clip_no_pause_normalized\")\n",
    "df_mean_var = df_train_100ms_labeled_normalized.groupby(\"audiobook_name is_narrative\".split()).apply(lambda x: pd.Series([*calc_mean_var_pause_length(x)], index=[\"mean\", \"var\"])).reset_index()\n",
    "plt.scatter(df_mean_var[\"mean\"], df_mean_var[\"var\"], label=\"groupby (audiobook, is_narrative)\", color=\"blue\")\n",
    "\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled, \"speaker\", \"morp_pause_clip_no_pause_normalized\")\n",
    "df_mean_var = df_train_100ms_labeled_normalized.groupby(\"speaker\").apply(lambda x: pd.Series([*calc_mean_var_pause_length(x)], index=[\"mean\", \"var\"])).reset_index()\n",
    "plt.scatter(df_mean_var[\"mean\"], df_mean_var[\"var\"], label=\"groupby speaker\", color=\"purple\")\n",
    "\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled, \"book\", \"morp_pause_clip_no_pause_normalized\")\n",
    "df_mean_var = df_train_100ms_labeled_normalized.groupby(\"book\").apply(lambda x: pd.Series([*calc_mean_var_pause_length(x)], index=[\"mean\", \"var\"])).reset_index()\n",
    "plt.scatter(df_mean_var[\"mean\"], df_mean_var[\"var\"], label=\"groupby book\", color=\"brown\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正規化後の分布をプロットする関数\n",
    "\n",
    "lims = (-2.5, 2.5)\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "# plt.suptitle(\"Normalized Distribution of 80ms Pauses\", fontsize=26)\n",
    "plt.subplot(611)\n",
    "plt.title(\"normalized by all\", fontsize=24)\n",
    "df_train_80ms_labeled_normalized = normalize_pause_length_with_group(df_train_80ms_labeled, \"\", \"morp_pause_clip_no_pause_normalized\")\n",
    "pause_lengths = df_train_80ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "plt.hist(pause_lengths, bins=100, alpha=0.5, label='all', color=\"orange\")\n",
    "plt.xlim(*lims)\n",
    "# plt.xticks(np.arange(-2.5, 2.6, 0.5)) \n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "# plt.legend()\n",
    "\n",
    "plt.subplot(612)\n",
    "plt.title(\"normalized by audiobook\", fontsize=24)\n",
    "df_train_80ms_labeled_normalized = normalize_pause_length_with_group(df_train_80ms_labeled, \"audiobook_name\", \"morp_pause_clip_no_pause_normalized\")\n",
    "pause_lengths = df_train_80ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "plt.hist(pause_lengths, bins=100, alpha=0.5, label='audiobook', color=\"green\")\n",
    "plt.xlim(*lims)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "# plt.legend()\n",
    "\n",
    "plt.subplot(613)\n",
    "plt.title(\"normalized by narrative\", fontsize=24)\n",
    "df_train_80ms_labeled_normalized = normalize_pause_length_with_group(df_train_80ms_labeled, \"is_narrative\", \"morp_pause_clip_no_pause_normalized\")\n",
    "pause_lengths = df_train_80ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "plt.hist(pause_lengths, bins=100, alpha=0.5, label='is_narrative', color=\"blue\")\n",
    "plt.xlim(lims)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "# plt.legend()\n",
    "\n",
    "plt.subplot(614)\n",
    "plt.title(\"normalized by audiobook-narrative\", fontsize=24)\n",
    "df_train_80ms_labeled_normalized = normalize_pause_length_with_group(df_train_80ms_labeled, \"audiobook_name is_narrative\".split(), \"morp_pause_clip_no_pause_normalized\")\n",
    "pause_lengths = df_train_80ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "plt.hist(pause_lengths, bins=100, alpha=0.5, label='(audiobook, is_narrative)', color=\"red\")\n",
    "plt.xlim(*lims)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "# plt.legend()\n",
    "\n",
    "plt.subplot(615)\n",
    "plt.title(\"normalized by speaker\", fontsize=24)\n",
    "df_train_80ms_labeled_normalized = normalize_pause_length_with_group(df_train_80ms_labeled, \"speaker\", \"morp_pause_clip_no_pause_normalized\")\n",
    "pause_lengths = df_train_80ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "plt.hist(pause_lengths, bins=100, alpha=0.5, label='speaker', color=\"purple\")\n",
    "plt.xlim(*lims)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "# plt.legend()\n",
    "\n",
    "plt.subplot(616)\n",
    "plt.title(\"normalized by book\", fontsize=24)\n",
    "df_train_80ms_labeled_normalized = normalize_pause_length_with_group(df_train_80ms_labeled, \"book\", \"morp_pause_clip_no_pause_normalized\")\n",
    "pause_lengths = df_train_80ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "plt.hist(pause_lengths, bins=100, alpha=0.5, label='book', color=\"brown\")\n",
    "plt.xlim(*lims)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.xlabel(\"pause duration (s)\", fontsize=20)\n",
    "# plt.legend()\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) \n",
    "# plt.tight_layout(rect=[0, 0.05, 1, 0.90])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正規化後の分布をプロットする関数\n",
    "\n",
    "lims = (-2.5, 2.5)\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "# plt.suptitle(\"Normalized Distribution of 100ms Pauses\", fontsize=26)\n",
    "plt.subplot(611)\n",
    "plt.title(\"normalized by all\", fontsize=24)\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled, \"\", \"morp_pause_clip_no_pause_normalized\")\n",
    "pause_lengths = df_train_100ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "plt.hist(pause_lengths, bins=100, alpha=0.5, label='all', color=\"orange\")\n",
    "plt.xlim(*lims)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "plt.subplot(612)\n",
    "plt.title(\"normalized by audiobook\", fontsize=24)\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled, \"audiobook_name\", \"morp_pause_clip_no_pause_normalized\")\n",
    "pause_lengths = df_train_100ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "plt.hist(pause_lengths, bins=100, alpha=0.5, label='audiobook', color=\"green\")\n",
    "plt.xlim(*lims)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "plt.subplot(613)\n",
    "plt.title(\"normalized by narrative\", fontsize=24)\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled, \"is_narrative\", \"morp_pause_clip_no_pause_normalized\")\n",
    "pause_lengths = df_train_100ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "plt.hist(pause_lengths, bins=100, alpha=0.5, label='is_narrative', color=\"blue\")\n",
    "plt.xlim(lims)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "plt.subplot(614)\n",
    "plt.title(\"normalized by audiobook-narrative\", fontsize=24)\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled, \"audiobook_name is_narrative\".split(), \"morp_pause_clip_no_pause_normalized\")\n",
    "pause_lengths = df_train_100ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "plt.hist(pause_lengths, bins=100, alpha=0.5, label='(audiobook, is_narrative)', color=\"red\")\n",
    "plt.xlim(*lims)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "plt.subplot(615)\n",
    "plt.title(\"normalized by speaker\", fontsize=24)\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled, \"speaker\", \"morp_pause_clip_no_pause_normalized\")\n",
    "pause_lengths = df_train_100ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "plt.hist(pause_lengths, bins=100, alpha=0.5, label='speaker', color=\"purple\")\n",
    "plt.xlim(*lims)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "plt.subplot(616)\n",
    "plt.title(\"normalized by book\", fontsize=24)\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled, \"book\", \"morp_pause_clip_no_pause_normalized\")\n",
    "pause_lengths = df_train_100ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "plt.hist(pause_lengths, bins=100, alpha=0.5, label='book', color=\"brown\")\n",
    "plt.xlim(*lims)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.xlabel(\"pause duration (s) \", fontsize=24)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正規化後の分布をプロットする関数\n",
    "\n",
    "lims = (-2.5, 2.5)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "# plt.suptitle(\"Normalized Distribution of 100ms Pauses\", fontsize=26)\n",
    "plt.subplot(311)\n",
    "plt.title(\"normalized by all\", fontsize=24)\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled, \"\", \"morp_pause_clip_no_pause_normalized\")\n",
    "pause_lengths = df_train_100ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "plt.hist(pause_lengths, bins=100, alpha=0.5, label='all', color=\"orange\")\n",
    "plt.xlim(*lims)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.title(\"normalized by audiobook\", fontsize=24)\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled, \"audiobook_name\", \"morp_pause_clip_no_pause_normalized\")\n",
    "pause_lengths = df_train_100ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "plt.hist(pause_lengths, bins=100, alpha=0.5, label='audiobook', color=\"green\")\n",
    "plt.xlim(*lims)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "# plt.subplot(613)\n",
    "# plt.title(\"normalized by narrative\", fontsize=24)\n",
    "# df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled, \"is_narrative\", \"morp_pause_clip_no_pause_normalized\")\n",
    "# pause_lengths = df_train_100ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "# plt.hist(pause_lengths, bins=100, alpha=0.5, label='is_narrative', color=\"blue\")\n",
    "# plt.xlim(lims)\n",
    "# plt.xticks(fontsize=18)\n",
    "# plt.yticks(fontsize=18)\n",
    "\n",
    "# plt.subplot(614)\n",
    "# plt.title(\"normalized by audiobook-narrative\", fontsize=24)\n",
    "# df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled, \"audiobook_name is_narrative\".split(), \"morp_pause_clip_no_pause_normalized\")\n",
    "# pause_lengths = df_train_100ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "# plt.hist(pause_lengths, bins=100, alpha=0.5, label='(audiobook, is_narrative)', color=\"red\")\n",
    "# plt.xlim(*lims)\n",
    "# plt.xticks(fontsize=18)\n",
    "# plt.yticks(fontsize=18)\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.title(\"normalized by speaker\", fontsize=24)\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled, \"speaker\", \"morp_pause_clip_no_pause_normalized\")\n",
    "pause_lengths = df_train_100ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "plt.hist(pause_lengths, bins=100, alpha=0.5, label='speaker', color=\"purple\")\n",
    "plt.xlim(*lims)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "# plt.subplot(616)\n",
    "# plt.title(\"normalized by book\", fontsize=24)\n",
    "# df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled, \"book\", \"morp_pause_clip_no_pause_normalized\")\n",
    "# pause_lengths = df_train_100ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "# plt.hist(pause_lengths, bins=100, alpha=0.5, label='book', color=\"brown\")\n",
    "# plt.xlim(*lims)\n",
    "# plt.xticks(fontsize=18)\n",
    "# plt.yticks(fontsize=18)\n",
    "plt.xlabel(\"pause duration (s) \", fontsize=24)\n",
    "\n",
    "# plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "plt.savefig(\"100ms_normalized_pause_distribution.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正規化後の分布をプロットする関数\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.suptitle(\"100ms pauses normalized\", fontsize=20)\n",
    "plt.subplot(611)\n",
    "plt.title(\"100ms normalized by all\")\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled, \"\", \"morp_pause_clip_no_pause_normalized\")\n",
    "pause_lengths = df_train_100ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "plt.hist(pause_lengths, bins=100, alpha=0.5, label='all', color=\"orange\")\n",
    "plt.xlim(-6, 6)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(612)\n",
    "plt.title(\"100ms normalized by audiobook\")\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled, \"audiobook_name\", \"morp_pause_clip_no_pause_normalized\")\n",
    "pause_lengths = df_train_100ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "plt.hist(pause_lengths, bins=100, alpha=0.5, label='audiobook', color=\"green\")\n",
    "plt.xlim(-6, 6)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(613)\n",
    "plt.title(\"100ms normalized by is_narrative\")\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled, \"is_narrative\", \"morp_pause_clip_no_pause_normalized\")\n",
    "pause_lengths = df_train_100ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "plt.hist(pause_lengths, bins=100, alpha=0.5, label='is_narrative', color=\"blue\")\n",
    "plt.xlim(-6, 6)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(614)\n",
    "plt.title(\"100ms normalized by (audiobook, is_narrative)\")\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled, \"audiobook_name is_narrative\".split(), \"morp_pause_clip_no_pause_normalized\")\n",
    "pause_lengths = df_train_100ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "plt.hist(pause_lengths, bins=100, alpha=0.5, label='(audiobook, is_narrative)', color=\"red\")\n",
    "plt.xlim(-6, 6)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(615)\n",
    "plt.title(\"100ms normalized by speaker\")\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled, \"speaker\", \"morp_pause_clip_no_pause_normalized\")\n",
    "pause_lengths = df_train_100ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "plt.hist(pause_lengths, bins=100, alpha=0.5, label='speaker', color=\"purple\")\n",
    "plt.xlim(-6, 6)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(616)\n",
    "plt.title(\"100ms normalized by book\")\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled, \"book\", \"morp_pause_clip_no_pause_normalized\")\n",
    "pause_lengths = df_train_100ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized\"].apply(extract_pause_lengths_in_sentence).explode().astype(float)\n",
    "plt.hist(pause_lengths, bins=100, alpha=0.5, label='book', color=\"brown\")\n",
    "plt.xlim(-6, 6)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT学習データに、正規化したポーズ長のデータを追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audiobook_narraiveのグループのうち、数が少ないものを除外する関数\n",
    "def exclude_few_audiobook_narratives(df: pd.DataFrame, threshold: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"audiobook_narraiveのグループのうち、数が少ないものを除外する関数\"\"\"\n",
    "    df[\"audiobook_narrative\"] = df[\"audiobook_name\"] + \"_\" + df[\"is_narrative\"].astype(str)\n",
    "    audiobook_narrative_counts = df[\"audiobook_narrative\"].value_counts()\n",
    "    audiobook_narratives_to_exclude = audiobook_narrative_counts[audiobook_narrative_counts < threshold].index\n",
    "    print(f\"audiobook_narratives_to_exclude: {audiobook_narratives_to_exclude}\")\n",
    "    return df[~df[\"audiobook_narrative\"].isin(audiobook_narratives_to_exclude)]\n",
    "\n",
    "# train_test_splitを行って、train, val, testの列を追加する関数\n",
    "def train_val_test_split(df: pd.DataFrame, test_size: float = 0.2, val_size: float = 0.25) -> pd.DataFrame:\n",
    "    \"\"\"train_test_splitを行って、train, val, testの列を追加する関数\"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    stratify_col = df[\"audiobook_name\"] + \"_\" + df[\"is_narrative\"].astype(str)\n",
    "    df_train, df_test = train_test_split(df, test_size=test_size, random_state=42, stratify=stratify_col)\n",
    "    # stratify_colをdf_trainに基づいて再計算\n",
    "    stratify_col_train = df_train[\"audiobook_name\"] + \"_\" + df_train[\"is_narrative\"].astype(str)\n",
    "    df_train, df_val = train_test_split(df_train, test_size=val_size, random_state=42, stratify=stratify_col_train)\n",
    "    df_train[\"train_val_test\"] = \"train\"\n",
    "    df_val[\"train_val_test\"] = \"val\"\n",
    "    df_test[\"train_val_test\"] = \"test\"\n",
    "    assert len(set(df_train.index) & set(df_val.index)) == 0, \"train and val have common index\"\n",
    "    assert len(set(df_train.index) & set(df_test.index)) == 0, \"train and test have common index\"\n",
    "    assert len(set(df_val.index) & set(df_test.index)) == 0, \"val and test have common index\"\n",
    "    return pd.concat([df_train, df_val, df_test])\n",
    "\n",
    "# 訓練データのみで平均と分散を計算して、それを使って正規化する関数\n",
    "def normalize_pause_length_with_train(df_train: pd.DataFrame, df: pd.DataFrame, group: str, added_col: str) -> pd.DataFrame:\n",
    "    \"\"\"訓練データのみで平均と分散を計算して、それを使って正規化する関数\"\"\"\n",
    "    df_normalized = df.copy()\n",
    "    if group == \"none\":\n",
    "        # mean, var = calc_mean_var_pause_length(df_train)\n",
    "        df_normalized[\"mean\"] = 0\n",
    "        df_normalized[\"var\"] = 1\n",
    "    elif group == \"all\":\n",
    "        mean, var = calc_mean_var_pause_length(df_train)\n",
    "        df_normalized[\"mean\"] = mean\n",
    "        df_normalized[\"var\"] = var\n",
    "    else:\n",
    "        df_mean_var = df_train.groupby(group).apply(lambda x: pd.Series([*calc_mean_var_pause_length(x)], index=[\"mean\", \"var\"])).reset_index()\n",
    "        # 分散が0になるもの確認\n",
    "        # print(df_mean_var[df_mean_var[\"var\"] <= 0])\n",
    "        df_normalized = pd.merge(df_normalized, df_mean_var, on=group, how=\"left\")\n",
    "    \n",
    "    df_normalized[added_col] = df_normalized.apply(lambda x: normalize_pause_lengths_in_texts(x[\"morp_pause_clip_no_pause\"], x[\"mean\"], x[\"var\"]), axis=1)\n",
    "    rename_dict = {\"none\": \"none\", \"all\": \"all\", \"audiobook_name\": \"audiobook\", \"is_narrative\": \"narrative\", \"audiobook_name_is_narrative\": \"audiobook_narrative\", \"speaker\": \"speaker\", \"book\": \"book\"}\n",
    "    rename_group = rename_dict[group] if type(group) == str else \"_\".join([rename_dict[g] for g in group])\n",
    "    df_normalized.rename(columns={\"mean\": f\"mean_{rename_group}\", \"var\": f\"var_{rename_group}\"}, inplace=True)\n",
    "    return df_normalized\n",
    "\n",
    "# train, val, testごとにグループ数を計測する関数\n",
    "def count_groups_in_train_val_test(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"train, val, testごとにグループ数を計測する関数\"\"\"\n",
    "    nunique_audiobook_name = df.groupby(\"train_val_test\").apply(lambda x: x[\"audiobook_name\"].nunique())\n",
    "    nunique_is_narrative = df.groupby(\"train_val_test\").apply(lambda x: x[\"is_narrative\"].nunique())\n",
    "    df_tmp = df.copy()\n",
    "    df_tmp[\"audiobook_name_is_narrative\"] = df_tmp[\"audiobook_name\"] + \"_\" + df_tmp[\"is_narrative\"].astype(str)\n",
    "    nunique_audiobook_name_is_narrative = df_tmp.groupby(\"train_val_test\").apply(lambda x: x[\"audiobook_name_is_narrative\"].nunique())\n",
    "    nunique_speaker = df.groupby(\"train_val_test\").apply(lambda x: x[\"speaker\"].nunique())\n",
    "    nunique_book = df.groupby(\"train_val_test\").apply(lambda x: x[\"book\"].nunique())\n",
    "    group_counts = pd.concat([nunique_audiobook_name, nunique_is_narrative, nunique_audiobook_name_is_narrative, nunique_speaker, nunique_book], axis=1)\n",
    "    group_counts.columns = [\"audiobook_name\", \"is_narrative\", \"audiobook_name_is_narrative\", \"speaker\", \"book\"]\n",
    "    return group_counts\n",
    "\n",
    "#　長さが0のテキストの行を削除する関数\n",
    "def drop_zero_length_texts(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"長さが0のテキストの行を削除する関数\"\"\"\n",
    "    df[\"text_length\"] = df[\"morp_pause_clip_no_pause\"].apply(lambda x: len(x))\n",
    "    print(f\"length 0 texts: {len(df[df['text_length'] == 0])}\")\n",
    "    df = df[df[\"text_length\"] > 0]\n",
    "    return df.drop(\"text_length\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_80ms_path = exp_dir / \"bert_traindata_pause_position_with_length_wo_sokuon_80ms_normalized.pkl\"\n",
    "output_100ms_path = exp_dir / \"bert_traindata_pause_position_with_length_wo_sokuon_100ms_normalized.pkl\"\n",
    "\n",
    "# 長さが0のテキストの行を削除\n",
    "df_train_80ms_labeled = drop_zero_length_texts(df_train_80ms_labeled)\n",
    "df_train_100ms_labeled = drop_zero_length_texts(df_train_100ms_labeled)\n",
    "\n",
    "# グループごとにデータが少ないものを除外\n",
    "df_train_80ms_labeled = exclude_few_audiobook_narratives(df_train_80ms_labeled)\n",
    "df_train_100ms_labeled = exclude_few_audiobook_narratives(df_train_100ms_labeled)\n",
    "\n",
    "# train, val, testの列を追加\n",
    "df_train_80ms_labeled = train_val_test_split(df_train_80ms_labeled)\n",
    "df_train_100ms_labeled = train_val_test_split(df_train_100ms_labeled)\n",
    "\n",
    "# グループごとの数を計測\n",
    "print(\"group counts\")\n",
    "print(\"80ms\")\n",
    "display(count_groups_in_train_val_test(df_train_80ms_labeled))\n",
    "print(\"100ms\")\n",
    "display(count_groups_in_train_val_test(df_train_100ms_labeled))\n",
    "\n",
    "# 80ms, 正規化したデータを保存\n",
    "df_train_80ms_labeled_normalized = normalize_pause_length_with_train(df_train_80ms_labeled[df_train_80ms_labeled[\"train_val_test\"] == \"train\"], df_train_80ms_labeled, \"none\", \"morp_pause_clip_no_pause_normalized_80ms_none\")\n",
    "df_train_80ms_labeled_normalized = normalize_pause_length_with_train(df_train_80ms_labeled[df_train_80ms_labeled[\"train_val_test\"] == \"train\"], df_train_80ms_labeled_normalized, \"all\", \"morp_pause_clip_no_pause_normalized_80ms_all\")\n",
    "df_train_80ms_labeled_normalized = normalize_pause_length_with_train(df_train_80ms_labeled[df_train_80ms_labeled[\"train_val_test\"] == \"train\"], df_train_80ms_labeled_normalized, \"audiobook_name\", \"morp_pause_clip_no_pause_normalized_80ms_audiobook\")\n",
    "df_train_80ms_labeled_normalized = normalize_pause_length_with_train(df_train_80ms_labeled[df_train_80ms_labeled[\"train_val_test\"] == \"train\"], df_train_80ms_labeled_normalized, \"is_narrative\", \"morp_pause_clip_no_pause_normalized_80ms_narrative\")\n",
    "df_train_80ms_labeled_normalized = normalize_pause_length_with_train(df_train_80ms_labeled[df_train_80ms_labeled[\"train_val_test\"] == \"train\"], df_train_80ms_labeled_normalized, \"audiobook_name is_narrative\".split(), \"morp_pause_clip_no_pause_normalized_80ms_audiobook_narrative\")\n",
    "df_train_80ms_labeled_normalized = normalize_pause_length_with_train(df_train_80ms_labeled[df_train_80ms_labeled[\"train_val_test\"] == \"train\"], df_train_80ms_labeled_normalized, \"speaker\", \"morp_pause_clip_no_pause_normalized_80ms_speaker\")\n",
    "df_train_80ms_labeled_normalized = normalize_pause_length_with_train(df_train_80ms_labeled[df_train_80ms_labeled[\"train_val_test\"] == \"train\"], df_train_80ms_labeled_normalized, \"book\", \"morp_pause_clip_no_pause_normalized_80ms_book\")\n",
    "display(df_train_80ms_labeled_normalized.head())\n",
    "df_train_80ms_labeled_normalized.to_pickle(output_80ms_path)\n",
    "print(f\"save: {output_80ms_path}\")\n",
    "\n",
    "# 全てのデータで平均と分散を計算して、それを使って正規化する場合(リークするため、20240208コメントアウト)\n",
    "# df_train_80ms_labeled_normalized = normalize_pause_length_with_group(df_train_80ms_labeled, \"\", \"morp_pause_clip_no_pause_normalized_80ms_all\")\n",
    "# df_train_80ms_labeled_normalized = normalize_pause_length_with_group(df_train_80ms_labeled_normalized, \"audiobook_name\", \"morp_pause_clip_no_pause_normalized_80ms_audiobook\")\n",
    "# df_train_80ms_labeled_normalized = normalize_pause_length_with_group(df_train_80ms_labeled_normalized, \"is_narrative\", \"morp_pause_clip_no_pause_normalized_80ms_narrative\")\n",
    "# df_train_80ms_labeled_normalized = normalize_pause_length_with_group(df_train_80ms_labeled_normalized, \"audiobook_name is_narrative\".split(), \"morp_pause_clip_no_pause_normalized_80ms_audiobook_narrative\")\n",
    "# df_train_80ms_labeled_normalized = normalize_pause_length_with_group(df_train_80ms_labeled_normalized, \"speaker\", \"morp_pause_clip_no_pause_normalized_80ms_speaker\")\n",
    "# df_train_80ms_labeled_normalized = normalize_pause_length_with_group(df_train_80ms_labeled_normalized, \"book\", \"morp_pause_clip_no_pause_normalized_80ms_book\")\n",
    "# display(df_train_80ms_labeled_normalized.head())\n",
    "# df_train_80ms_labeled_normalized.to_pickle(output_80ms_path)\n",
    "# print(f\"save: {output_80ms_path}\")\n",
    "\n",
    "# 100ms, 正規化したデータを保存\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_train(df_train_100ms_labeled[df_train_100ms_labeled[\"train_val_test\"] == \"train\"], df_train_100ms_labeled, \"none\", \"morp_pause_clip_no_pause_normalized_100ms_none\")\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_train(df_train_100ms_labeled[df_train_100ms_labeled[\"train_val_test\"] == \"train\"], df_train_100ms_labeled_normalized, \"all\", \"morp_pause_clip_no_pause_normalized_100ms_all\")\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_train(df_train_100ms_labeled[df_train_100ms_labeled[\"train_val_test\"] == \"train\"], df_train_100ms_labeled_normalized, \"audiobook_name\", \"morp_pause_clip_no_pause_normalized_100ms_audiobook\")\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_train(df_train_100ms_labeled[df_train_100ms_labeled[\"train_val_test\"] == \"train\"], df_train_100ms_labeled_normalized, \"is_narrative\", \"morp_pause_clip_no_pause_normalized_100ms_narrative\")\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_train(df_train_100ms_labeled[df_train_100ms_labeled[\"train_val_test\"] == \"train\"], df_train_100ms_labeled_normalized, \"audiobook_name is_narrative\".split(), \"morp_pause_clip_no_pause_normalized_100ms_audiobook_narrative\")\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_train(df_train_100ms_labeled[df_train_100ms_labeled[\"train_val_test\"] == \"train\"], df_train_100ms_labeled_normalized, \"speaker\", \"morp_pause_clip_no_pause_normalized_100ms_speaker\")\n",
    "df_train_100ms_labeled_normalized = normalize_pause_length_with_train(df_train_100ms_labeled[df_train_100ms_labeled[\"train_val_test\"] == \"train\"], df_train_100ms_labeled_normalized, \"book\", \"morp_pause_clip_no_pause_normalized_100ms_book\")\n",
    "display(df_train_100ms_labeled_normalized.head())\n",
    "df_train_100ms_labeled_normalized.to_pickle(output_100ms_path)\n",
    "print(f\"save: {output_100ms_path}\")\n",
    "\n",
    "\n",
    "# df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled, \"\", \"morp_pause_clip_no_pause_normalized_100ms_all\")\n",
    "# df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled_normalized, \"audiobook_name\", \"morp_pause_clip_no_pause_normalized_100ms_audiobook\")\n",
    "# df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled_normalized, \"is_narrative\", \"morp_pause_clip_no_pause_normalized_100ms_narrative\")\n",
    "# df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled_normalized, \"audiobook_name is_narrative\".split(), \"morp_pause_clip_no_pause_normalized_100ms_audiobook_narrative\")\n",
    "# df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled_normalized, \"speaker\", \"morp_pause_clip_no_pause_normalized_100ms_speaker\")\n",
    "# df_train_100ms_labeled_normalized = normalize_pause_length_with_group(df_train_100ms_labeled_normalized, \"book\", \"morp_pause_clip_no_pause_normalized_100ms_book\")\n",
    "# display(df_train_100ms_labeled_normalized.head())\n",
    "# df_train_100ms_labeled_normalized.to_pickle(output_100ms_path)\n",
    "# print(f\"save: {output_100ms_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの行数を確認\n",
    "print(\"row counts\")\n",
    "print(\"80ms\")\n",
    "display(df_train_80ms_labeled_normalized[\"train_val_test\"].value_counts())\n",
    "print(\"100ms\")\n",
    "display(df_train_100ms_labeled_normalized[\"train_val_test\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colをsortして表示\n",
    "display(df_train_80ms_labeled_normalized.columns.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# morpを見る noneが0,1で正規化されているか確認\n",
    "display(df_train_80ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized_80ms_none\"].head())\n",
    "display(df_train_80ms_labeled_normalized[\"morp_pause_clip_no_pause\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# morpが長さ0のものを見る\n",
    "display(df_train_80ms_labeled_normalized[df_train_80ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized_80ms_none\"].apply(len) == 0])\n",
    "# 長さ0がnone, all, audiobook, is_narrative, audiobook_narrative, speaker, bookでそれぞれ同じか確認\n",
    "display(df_train_80ms_labeled_normalized[df_train_80ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized_80ms_none\"].apply(len) == 0][\"morp_pause_clip_no_pause_normalized_80ms_none\"].equals(df_train_80ms_labeled_normalized[df_train_80ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized_80ms_all\"].apply(len) == 0][\"morp_pause_clip_no_pause_normalized_80ms_all\"]))\n",
    "display(df_train_80ms_labeled_normalized[df_train_80ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized_80ms_none\"].apply(len) == 0][\"morp_pause_clip_no_pause_normalized_80ms_none\"].equals(df_train_80ms_labeled_normalized[df_train_80ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized_80ms_audiobook\"].apply(len) == 0][\"morp_pause_clip_no_pause_normalized_80ms_audiobook\"]))\n",
    "display(df_train_80ms_labeled_normalized[df_train_80ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized_80ms_none\"].apply(len) == 0][\"morp_pause_clip_no_pause_normalized_80ms_none\"].equals(df_train_80ms_labeled_normalized[df_train_80ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized_80ms_narrative\"].apply(len) == 0][\"morp_pause_clip_no_pause_normalized_80ms_narrative\"]))\n",
    "display(df_train_80ms_labeled_normalized[df_train_80ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized_80ms_none\"].apply(len) == 0][\"morp_pause_clip_no_pause_normalized_80ms_none\"].equals(df_train_80ms_labeled_normalized[df_train_80ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized_80ms_audiobook_narrative\"].apply(len) == 0][\"morp_pause_clip_no_pause_normalized_80ms_audiobook_narrative\"]))\n",
    "display(df_train_80ms_labeled_normalized[df_train_80ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized_80ms_none\"].apply(len) == 0][\"morp_pause_clip_no_pause_normalized_80ms_none\"].equals(df_train_80ms_labeled_normalized[df_train_80ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized_80ms_speaker\"].apply(len) == 0][\"morp_pause_clip_no_pause_normalized_80ms_speaker\"]))\n",
    "display(df_train_80ms_labeled_normalized[df_train_80ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized_80ms_none\"].apply(len) == 0][\"morp_pause_clip_no_pause_normalized_80ms_none\"].equals(df_train_80ms_labeled_normalized[df_train_80ms_labeled_normalized[\"morp_pause_clip_no_pause_normalized_80ms_book\"].apply(len) == 0][\"morp_pause_clip_no_pause_normalized_80ms_book\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jvs_data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
