{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTモデルの学習のために、最適なmax_lengthを計算する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文中ポーズのテキストのサブトークンの長さを計算する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ作成：/home/takeshun256/PausePrediction/research/preprocess_jmac/学習データの作成.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/takeshun256/PausePrediction\")\n",
    "\n",
    "# import own library\n",
    "from config import DATA_DIR, DATA_TAKESHUN256_DIR, SRC_DIR, DATA_IN_ROOT_DIR\n",
    "\n",
    "# define path\n",
    "corpus_name = \"jmac\"\n",
    "exp_name = \"03_VAD_Adjusted\"\n",
    "exp_dir = Path(DATA_TAKESHUN256_DIR) / corpus_name / exp_name\n",
    "audiobook_yaml_path = Path(DATA_IN_ROOT_DIR) / corpus_name / \"text_audio_dict_new.yaml\"\n",
    "\n",
    "assert exp_dir.exists()\n",
    "assert audiobook_yaml_path.exists()\n",
    "\n",
    "# audio book data\n",
    "with open(audiobook_yaml_path, \"rb\") as f:\n",
    "    audiobook_dict = yaml.safe_load(f)\n",
    "\n",
    "# データの一覧\n",
    "pause_time_threshold_mss = [80, 100]\n",
    "preprocess_types = [\"none\", \"all\", \"audiobook\", \"narrative\", \"audiobook_narrative\"]\n",
    "num_labels = [1, 2]\n",
    "\n",
    "# output dir\n",
    "output_dir = exp_dir / \"data_bert\"\n",
    "assert output_dir.exists()\n",
    "\n",
    "\n",
    "print(\"audio book data\")\n",
    "print(len(audiobook_dict))\n",
    "pprint(audiobook_dict[list(audiobook_dict.keys())[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80ms, 100msのデータを読み込む\n",
    "df = pd.read_pickle(output_dir / f\"80ms\" / \"none\" / f\"bert_traindata_1label.pkl\")\n",
    "print(df.shape)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "# ながさ: cls + textにtokenzieかけたもの + sep  +++ max_lengthに満たない場合はpadding\n",
    "from transformers import BertJapaneseTokenizer\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
    "\n",
    "text_length_list = []\n",
    "for texts in tqdm(df[\"texts\"]):\n",
    "    text_lengh = 0\n",
    "    for text in texts:\n",
    "        text_lengh += len(tokenizer.tokenize(text))\n",
    "    text_lengh += 2  # cls, sep\n",
    "    text_length_list.append(text_lengh)\n",
    "\n",
    "# 分布を出す\n",
    "plt.hist(text_length_list, bins=100)\n",
    "\n",
    "# 5, 95パーセンタイルを出す\n",
    "print(f\"95%: {np.percentile(text_length_list, 95)}\")\n",
    "\n",
    "# 最大値を出す\n",
    "print(f\"max: {max(text_length_list)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文間ポーズのテキストのサブトークンの長さを計算する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80ms, 100msのデータを読み込む\n",
    "df = pd.read_pickle(output_dir / f\"80ms\" / \"none\" / f\"bert_traindata_BetweenSentences_1label.pkl\")\n",
    "print(df.shape)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "# ながさ: cls + textにtokenzieかけたもの + sep  +++ max_lengthに満たない場合はpadding\n",
    "from transformers import BertJapaneseTokenizer\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
    "\n",
    "text_length_list = []\n",
    "for texts in tqdm(df[\"texts\"]):\n",
    "    text_lengh = 0\n",
    "    if \"[SEP]\" not in texts:\n",
    "        raise ValueError(f\"not found [SEP] in {texts}\")\n",
    "    text1, text2 = texts.split(\"[SEP]\")\n",
    "    text_lengh += len(tokenizer.tokenize(text1))\n",
    "    text_lengh += len(tokenizer.tokenize(text2))\n",
    "    text_lengh += 3  # cls, sep, sep\n",
    "    text_length_list.append(text_lengh)\n",
    "\n",
    "# 分布を出す\n",
    "plt.hist(text_length_list, bins=100)\n",
    "\n",
    "# 5, 95パーセンタイルを出す\n",
    "print(f\"95%: {np.percentile(text_length_list, 95)}\")\n",
    "\n",
    "# 最大値を出す\n",
    "print(f\"max: {max(text_length_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyopenjtalk_julius",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
